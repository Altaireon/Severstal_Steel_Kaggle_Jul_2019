{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128x128.pth\t       unet_mobilenet2.pth  unet_se_resnext50_32x4d.pth\r\n",
      "resnet34_classify.pth  unet_resnet34.pth\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../input/severstalmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# ! python ../input/mlcomp/mlcomp/mlcomp/setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from tqdm import tqdm_notebook,tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.jit import load\n",
    "import ast\n",
    "from os.path import join\n",
    "from numbers import Number\n",
    "from collections import defaultdict\n",
    "from typing import Callable, Dict\n",
    "\n",
    "import tifffile\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# from mlcomp.contrib.transform.albumentations import ChannelTranspose\n",
    "# from mlcomp.contrib.dataset.classify import ImageDataset\n",
    "# from mlcomp.contrib.transform.rle import rle2mask, mask2rle\n",
    "# from mlcomp.contrib.transform.tta import TtaWrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# Get necessary Imports\n",
    "import pdb\n",
    "import torch.backends.cudnn as cudnn\n",
    "import albumentations as albu\n",
    "from albumentations import (Normalize, Compose)\n",
    "from albumentations.pytorch import ToTensor\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from albumentations import ImageOnlyTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/catalyst-team/mlcomp/blob/master/mlcomp/contrib/torch/tensors.py\n",
    "def flip(x, dim):\n",
    "    indices = [slice(None)] * x.dim()\n",
    "    indices[dim] = torch.arange(x.size(dim) - 1, -1, -1,\n",
    "                                dtype=torch.long, device=x.device)\n",
    "    return x[tuple(indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/catalyst-team/mlcomp/blob/master/mlcomp/contrib/transform/albumentations.py\n",
    "class ChannelTranspose(ImageOnlyTransform):\n",
    "    def get_transform_init_args_names(self):\n",
    "        return ()\n",
    "\n",
    "    def get_params_dependent_on_targets(self, params):\n",
    "        pass\n",
    "\n",
    "    def __init__(self, axes=(2, 0, 1)):\n",
    "        super().__init__(always_apply=True)\n",
    "        self.axes = axes\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        return np.transpose(img, self.axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "#https://github.com/catalyst-team/mlcomp/blob/master/mlcomp/contrib/transform/tta.py\n",
    "class TtaWrap(Dataset):\n",
    "    def __init__(self, dataset: Dataset, tfms=()):\n",
    "        self.dataset = dataset\n",
    "        self.tfms = tfms\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.dataset[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def inverse(self, a: np.array):\n",
    "        last_dim = len(a.shape) - 1\n",
    "        for t in self.tfms:\n",
    "            if isinstance(t, A.HorizontalFlip):\n",
    "                a = flip(a, last_dim)\n",
    "            elif isinstance(t, A.VerticalFlip):\n",
    "                a = flip(a, last_dim - 1)\n",
    "            elif isinstance(t, A.Transpose):\n",
    "                axis = (0, 1, 3, 2) if len(a.shape) == 4 else (0, 2, 1)\n",
    "                a = a.permute(*axis)\n",
    "\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            img_folder: str,\n",
    "            fold_csv: str = None,\n",
    "            fold_number: int = None,\n",
    "            is_test: bool = False,\n",
    "            gray_scale: bool = False,\n",
    "            num_classes=2,\n",
    "            max_count=None,\n",
    "            meta_cols=(),\n",
    "            transforms=None,\n",
    "            postprocess_func: Callable[[Dict], Dict] = None,\n",
    "            include_image_orig=False\n",
    "    ):\n",
    "        self.img_folder = img_folder\n",
    "\n",
    "        if fold_csv:\n",
    "            df = pd.read_csv(fold_csv)\n",
    "            if fold_number is not None:\n",
    "                if is_test:\n",
    "                    self.data = df[df['fold'] == fold_number]\n",
    "                else:\n",
    "                    self.data = df[df['fold'] != fold_number]\n",
    "            else:\n",
    "                self.data = df\n",
    "        else:\n",
    "            self.data = pd.DataFrame(\n",
    "                {'image': os.listdir(img_folder)}).sort_values(by='image')\n",
    "\n",
    "        self.data = self.data.to_dict(orient='row')\n",
    "        if max_count is not None:\n",
    "            self.apply_max_count(max_count)\n",
    "\n",
    "        for row in self.data:\n",
    "            self.preprocess_row(row)\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.gray_scale = gray_scale\n",
    "        self.num_classes = num_classes\n",
    "        self.meta_cols = meta_cols\n",
    "        self.postprocess_func = postprocess_func\n",
    "        self.include_image_orig = include_image_orig\n",
    "\n",
    "    def apply_max_count(self, max_count):\n",
    "        if isinstance(max_count, Number):\n",
    "            self.data = self.data[:max_count]\n",
    "        else:\n",
    "            data = defaultdict(list)\n",
    "            for row in self.data:\n",
    "                data[row['label']].append(row)\n",
    "            min_index = np.argmin(max_count)\n",
    "            min_count = len(data[min_index])\n",
    "            for k, v in data.items():\n",
    "                count = int(min_count * (max_count[k] / max_count[min_index]))\n",
    "                data[k] = data[k][:count]\n",
    "\n",
    "            self.data = [v for i in range(len(data)) for v in data[i]]\n",
    "\n",
    "    def preprocess_row(self, row: dict):\n",
    "        row['image'] = join(self.img_folder, row['image'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _get_item_before_transform(self, row: dict, item: dict):\n",
    "        pass\n",
    "\n",
    "    def _get_item_after_transform(self, row: dict,\n",
    "                                  transformed: dict,\n",
    "                                  res: dict):\n",
    "        if 'label' in row:\n",
    "            res['targets'] = ast.literal_eval(str(row['label']))\n",
    "            if isinstance(res['targets'], list):\n",
    "                res['targets'] = np.array(res['targets'], dtype=np.float32)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data[index]\n",
    "        image = self.read_image_file(row['image'], self.gray_scale)\n",
    "        item = {'image': image}\n",
    "\n",
    "        self._get_item_before_transform(row, item)\n",
    "\n",
    "        if self.transforms:\n",
    "            item = self.transforms(**item)\n",
    "        if self.gray_scale:\n",
    "            item['image'] = np.expand_dims(item['image'], axis=0)\n",
    "        res = {\n",
    "            'features': item['image'].astype(np.float32),\n",
    "            'image_file': row['image']\n",
    "        }\n",
    "        if self.include_image_orig:\n",
    "            res['image'] = image\n",
    "\n",
    "        for c in self.meta_cols:\n",
    "            res[c] = row[c]\n",
    "\n",
    "        self._get_item_after_transform(row, item, res)\n",
    "        if self.postprocess_func:\n",
    "            res = self.postprocess_func(res)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def read_image_file(path: str, gray_scale=False):\n",
    "        if path.endswith('.tiff') and not gray_scale:\n",
    "            return tifffile.imread(path)\n",
    "        elif path.endswith('.npy'):\n",
    "            return np.load(path)\n",
    "        else:\n",
    "            if gray_scale:\n",
    "                img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                assert img is not None, \\\n",
    "                    f'Image at path {path} does not exist'\n",
    "                return img.astype(np.uint8)\n",
    "            else:\n",
    "                img = cv2.imread(path)\n",
    "                assert img is not None, \\\n",
    "                    f'Image at path {path} does not exist'\n",
    "                return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_se_resnext50_32x4d = \\\n",
    "    load('/kaggle/input/severstalmodels/unet_se_resnext50_32x4d.pth').cuda()\n",
    "unet_mobilenet2 = load('/kaggle/input/severstalmodels/unet_mobilenet2.pth').cuda()\n",
    "unet_resnet34 = load('/kaggle/input/severstalmodels/unet_resnet34.pth').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        res = []\n",
    "        x = x.cuda()\n",
    "        with torch.no_grad():\n",
    "            for m in self.models:\n",
    "                res.append(m(x))\n",
    "#         res = torch.stack(res)\n",
    "#         return torch.mean(res, dim=0)\n",
    "        return res\n",
    "\n",
    "model = Model([unet_se_resnext50_32x4d, unet_mobilenet2, unet_resnet34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def create_transforms(additional):\n",
    "    res = list(additional)\n",
    "    # add necessary transformations\n",
    "    res.extend([\n",
    "        A.Normalize(\n",
    "            mean=(0.486, 0.457, 0.405), std=(0.230, 0.225, 0.224)\n",
    "        ),\n",
    "        ChannelTranspose()\n",
    "    ])\n",
    "    res = A.Compose(res)\n",
    "    return res\n",
    "\n",
    "img_folder = '/kaggle/input/severstal-steel-defect-detection/test_images'\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "\n",
    "# Different transforms for TTA wrapper\n",
    "transforms = [\n",
    "    [],\n",
    "    [A.HorizontalFlip(p=1)]\n",
    "#     [A.RandomCrop(256,800,p=1)]\n",
    "]\n",
    "\n",
    "transforms = [create_transforms(t) for t in transforms]\n",
    "datasets = [TtaWrap(ImageDataset(img_folder=img_folder, transforms=t), tfms=t) for t in transforms]\n",
    "loaders = [DataLoader(d, num_workers=num_workers, batch_size=batch_size, shuffle=False) for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "MYLIB = '../input/ssdefectlib/'\n",
    "WEIGHTS = '../input/ssdefectweights/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook,tqdm\n",
    "sys.path.append(MYLIB)\n",
    "from efficientnet import EfficientNetB5,BatchNorm2d,IMAGE_RGB_MEAN,IMAGE_RGB_STD,IS_GATHER_EXCITE,IS_PYTORCH_PAD\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class ConvBn2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, padding=1, stride=1):\n",
    "        super(ConvBn2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n",
    "        self.bn   = nn.BatchNorm2d(out_channel, eps=1e-5)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channel, channel, out_channel, stride=1, is_shortcut=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.is_shortcut = is_shortcut\n",
    "\n",
    "        self.conv_bn1 = ConvBn2d(in_channel,    channel, kernel_size=3, padding=1, stride=stride)\n",
    "        self.conv_bn2 = ConvBn2d(   channel,out_channel, kernel_size=3, padding=1, stride=1)\n",
    "\n",
    "        if is_shortcut:\n",
    "            self.shortcut = ConvBn2d(in_channel, out_channel, kernel_size=1, padding=0, stride=stride)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = F.relu(self.conv_bn1(x),inplace=True)\n",
    "        z = self.conv_bn2(z)\n",
    "\n",
    "        if self.is_shortcut:\n",
    "            x = self.shortcut(x)\n",
    "\n",
    "        z += x\n",
    "        z = F.relu(z,inplace=True)\n",
    "        return z\n",
    "\n",
    "class ResNet34(nn.Module):\n",
    "\n",
    "    def __init__(self, num_class=1000 ):\n",
    "        super(ResNet34, self).__init__()\n",
    "\n",
    "\n",
    "        self.block0  = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, padding=3, stride=2, bias=False),\n",
    "            BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.block1  = nn.Sequential(\n",
    "             nn.MaxPool2d(kernel_size=3, padding=1, stride=2),\n",
    "             BasicBlock( 64, 64, 64, stride=1, is_shortcut=False,),\n",
    "          * [BasicBlock( 64, 64, 64, stride=1, is_shortcut=False,) for i in range(1,3)],\n",
    "        )\n",
    "        self.block2  = nn.Sequential(\n",
    "             BasicBlock( 64,128,128, stride=2, is_shortcut=True, ),\n",
    "          * [BasicBlock(128,128,128, stride=1, is_shortcut=False,) for i in range(1,4)],\n",
    "        )\n",
    "        self.block3  = nn.Sequential(\n",
    "             BasicBlock(128,256,256, stride=2, is_shortcut=True, ),\n",
    "          * [BasicBlock(256,256,256, stride=1, is_shortcut=False,) for i in range(1,6)],\n",
    "        )\n",
    "        self.block4 = nn.Sequential(\n",
    "             BasicBlock(256,512,512, stride=2, is_shortcut=True, ),\n",
    "          * [BasicBlock(512,512,512, stride=1, is_shortcut=False,) for i in range(1,3)],\n",
    "        )\n",
    "        self.logit = nn.Linear(512,num_class)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = len(x)\n",
    "\n",
    "        x = self.block0(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n",
    "        logit = self.logit(x)\n",
    "        return logit\n",
    "    \n",
    "class Resnet34_classification(nn.Module):\n",
    "    def __init__(self,num_class=4):\n",
    "        super(Resnet34_classification, self).__init__()\n",
    "        e = ResNet34()\n",
    "        self.block = nn.ModuleList([\n",
    "            e.block0,\n",
    "            e.block1,\n",
    "            e.block2,\n",
    "            e.block3,\n",
    "            e.block4,\n",
    "        ])\n",
    "        e = None  #dropped\n",
    "        self.feature = nn.Conv2d(512,32, kernel_size=1) #dummy conv for dim reduction\n",
    "        self.logit = nn.Conv2d(32,num_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size,C,H,W = x.shape\n",
    "\n",
    "        for i in range( len(self.block)):\n",
    "            x = self.block[i](x)\n",
    "            #print(i, x.shape)\n",
    "\n",
    "        x = F.dropout(x,0.5,training=self.training)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = self.feature(x)\n",
    "        logit = self.logit(x)\n",
    "        return logit\n",
    "\n",
    "def getBibekClassifier():\n",
    "    model_classification = Resnet34_classification()\n",
    "    model_classification.load_state_dict(torch.load(WEIGHTS+'00007500_model.pth'))\n",
    "    model_classification = model_classification.cuda()\n",
    "    return model_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class ConvBn2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, dilation=1, stride=1, groups=1, is_bn=True):\n",
    "        super(ConvBn2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride, dilation=dilation, groups=groups, bias=False)\n",
    "        self.bn   = nn.BatchNorm2d(out_channels, eps=1e-5)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "class SqueezeExcite(nn.Module):\n",
    "    def __init__(self, in_channel, reduction=4, excite_size=-1):\n",
    "        super(SqueezeExcite, self).__init__()\n",
    "        self.excite_size=excite_size\n",
    "\n",
    "        self.fc1 = nn.Conv2d(in_channel, in_channel//reduction, kernel_size=1, padding=0)\n",
    "        self.fc2 = nn.Conv2d(in_channel//reduction, in_channel, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "\n",
    "        if IS_GATHER_EXCITE:\n",
    "            #print('IS_GATHER_EXCITE')\n",
    "            s = F.avg_pool2d(x, kernel_size=self.excite_size)\n",
    "        else:\n",
    "            s = F.adaptive_avg_pool2d(x,1)\n",
    "\n",
    "        s = self.fc1(s)\n",
    "        s = F.relu(s, inplace=True)\n",
    "        s = self.fc2(s)\n",
    "\n",
    "        if IS_GATHER_EXCITE:\n",
    "            s = F.interpolate(s, size=(x.shape[2],x.shape[3]), mode='nearest')\n",
    "\n",
    "        x = x*torch.sigmoid(s)\n",
    "        return x\n",
    "\n",
    "\n",
    "# bottleneck type C\n",
    "class SENextBottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_channel, channel, out_channel, stride=1, group=32, reduction=16, excite_size=-1, is_shortcut=False):\n",
    "        super(SENextBottleneckBlock, self).__init__()\n",
    "        self.is_shortcut = is_shortcut\n",
    "\n",
    "        self.conv_bn1 = ConvBn2d(in_channel,     channel, kernel_size=1, padding=0, stride=1)\n",
    "        self.conv_bn2 = ConvBn2d(   channel,     channel, kernel_size=3, padding=1, stride=stride, groups=group)\n",
    "        self.conv_bn3 = ConvBn2d(   channel, out_channel, kernel_size=1, padding=0, stride=1)\n",
    "        self.scale    = SqueezeExcite(out_channel, reduction, excite_size)\n",
    "\n",
    "        if is_shortcut:\n",
    "            self.shortcut = ConvBn2d(in_channel, out_channel, kernel_size=1, padding=0, stride=stride)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = F.relu(self.conv_bn1(x),inplace=True)\n",
    "        z = F.relu(self.conv_bn2(z),inplace=True)\n",
    "        z = self.scale(self.conv_bn3(z))\n",
    "\n",
    "        if self.is_shortcut:\n",
    "            z += self.shortcut(x)\n",
    "        else:\n",
    "            z += x\n",
    "\n",
    "        z = F.relu(z,inplace=True)\n",
    "        return z\n",
    "\n",
    "#resnext50_32x4d\n",
    "class ResNext50(nn.Module):\n",
    "\n",
    "    def __init__(self, num_class=1000 ):\n",
    "        super(ResNext50, self).__init__()\n",
    "\n",
    "\n",
    "        self.block0  = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "\n",
    "        self.block1  = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, padding=0, stride=2, ceil_mode=True),\n",
    "             SENextBottleneckBlock( 64, 128, 256, stride=1, is_shortcut=True,  excite_size=64),\n",
    "          * [SENextBottleneckBlock(256, 128, 256, stride=1, is_shortcut=False, excite_size=64) for i in range(1,3)],\n",
    "        )\n",
    "\n",
    "        self.block2  = nn.Sequential(\n",
    "             SENextBottleneckBlock(256, 256, 512, stride=2, is_shortcut=True,  excite_size=32),\n",
    "          * [SENextBottleneckBlock(512, 256, 512, stride=1, is_shortcut=False, excite_size=32) for i in range(1,4)],\n",
    "        )\n",
    "\n",
    "        self.block3  = nn.Sequential(\n",
    "             SENextBottleneckBlock( 512,512,1024, stride=2, is_shortcut=True,  excite_size=16),\n",
    "          * [SENextBottleneckBlock(1024,512,1024, stride=1, is_shortcut=False, excite_size=16) for i in range(1,6)],\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "             SENextBottleneckBlock(1024,1024,2048, stride=2, is_shortcut=True,  excite_size=8),\n",
    "          * [SENextBottleneckBlock(2048,1024,2048, stride=1, is_shortcut=False, excite_size=8) for i in range(1,3)],\n",
    "        )\n",
    "\n",
    "\n",
    "        self.logit = nn.Linear(2048,num_class)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = len(x)\n",
    "\n",
    "        x = self.block0(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n",
    "        logit = self.logit(x)\n",
    "        return logit\n",
    "\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self, num_class=4):\n",
    "        super(Net2, self).__init__()\n",
    "\n",
    "        e = ResNext50()\n",
    "        self.block0 = e.block0\n",
    "        self.block1 = e.block1\n",
    "        self.block2 = e.block2\n",
    "        self.block3 = e.block3\n",
    "        self.block4 = e.block4\n",
    "        e = None  #dropped\n",
    "\n",
    "        self.feature = nn.Conv2d(2048, 64, kernel_size=1) #dummy conv for dim reduction\n",
    "        self.logit   = nn.Conv2d(64, num_class, kernel_size=1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size,C,H,W = x.shape\n",
    "        x = x.clone()\n",
    "        x = x-torch.FloatTensor(IMAGE_RGB_MEAN).to(x.device).view(1,-1,1,1)\n",
    "        x = x/torch.FloatTensor(IMAGE_RGB_STD).to(x.device).view(1,-1,1,1)\n",
    "\n",
    "        x = self.block0(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "\n",
    "        x = F.dropout(x,0.5,training=self.training)\n",
    "#         x = F.avg_pool2d(x, kernel_size=(8, 13),stride=(8, 8))\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = self.feature(x)\n",
    "\n",
    "        logit = self.logit(x) #.view(batch_size,-1)\n",
    "        return logit\n",
    "    \n",
    "def getmyClassifier():\n",
    "    net = Net2()\n",
    "    net.load_state_dict(torch.load(WEIGHTS+'00059000_model.pth'))\n",
    "    net = net.cuda()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class SEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, reduction):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n",
    "                             padding=0)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n",
    "                             padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        module_input = x\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return module_input * x\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for bottlenecks that implements `forward()` method.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out = self.se_module(out) + residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SEBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    Bottleneck for SENet154.\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None):\n",
    "        super(SEBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes * 2)\n",
    "        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n",
    "                               stride=stride, padding=1, groups=groups,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes * 4)\n",
    "        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "class SEResNetBottleneck(Bottleneck):\n",
    "    \"\"\"\n",
    "    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n",
    "    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n",
    "    (the latter is used in the torchvision implementation of ResNet).\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n",
    "                 downsample=None):\n",
    "        super(SEResNetBottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n",
    "                               stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n",
    "                               groups=groups, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.se_module = SEModule(planes * 4, reduction=reduction)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "class SENet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n",
    "                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n",
    "                 downsample_padding=1, num_classes=1000):\n",
    "        \n",
    "        super(SENet, self).__init__()\n",
    "        self.inplanes = inplanes\n",
    "        if input_3x3:\n",
    "            layer0_modules = [\n",
    "                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn1', nn.BatchNorm2d(64)),\n",
    "                ('relu1', nn.ReLU(inplace=True)),\n",
    "                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn2', nn.BatchNorm2d(64)),\n",
    "                ('relu2', nn.ReLU(inplace=True)),\n",
    "                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n",
    "                                    bias=False)),\n",
    "                ('bn3', nn.BatchNorm2d(inplanes)),\n",
    "                ('relu3', nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        else:\n",
    "            layer0_modules = [\n",
    "                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n",
    "                                    padding=3, bias=False)),\n",
    "                ('bn1', nn.BatchNorm2d(inplanes)),\n",
    "                ('relu1', nn.ReLU(inplace=True)),\n",
    "            ]\n",
    "        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n",
    "                                                    ceil_mode=True)))\n",
    "        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n",
    "        self.layer1 = self._make_layer(\n",
    "            block,\n",
    "            planes=64,\n",
    "            blocks=layers[0],\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=1,\n",
    "            downsample_padding=0\n",
    "        )\n",
    "        self.layer2 = self._make_layer(\n",
    "            block,\n",
    "            planes=128,\n",
    "            blocks=layers[1],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.layer3 = self._make_layer(\n",
    "            block,\n",
    "            planes=256,\n",
    "            blocks=layers[2],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.layer4 = self._make_layer(\n",
    "            block,\n",
    "            planes=512,\n",
    "            blocks=layers[3],\n",
    "            stride=2,\n",
    "            groups=groups,\n",
    "            reduction=reduction,\n",
    "            downsample_kernel_size=downsample_kernel_size,\n",
    "            downsample_padding=downsample_padding\n",
    "        )\n",
    "        self.avg_pool = nn.AvgPool2d(7, stride=1)\n",
    "        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n",
    "        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n",
    "                    downsample_kernel_size=1, downsample_padding=0):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=downsample_kernel_size, stride=stride,\n",
    "                          padding=downsample_padding, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n",
    "                            downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups, reduction))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def features(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = self.avg_pool(x)\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.logits(x)\n",
    "        return x\n",
    "\n",
    "class SeResnet50Classification(nn.Module):\n",
    "    def __init__(self,num_class=4):\n",
    "        super(SeResnet50Classification, self).__init__()\n",
    "        e = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n",
    "                  dropout_p=None, inplanes=64, input_3x3=False,\n",
    "                  downsample_kernel_size=1, downsample_padding=0,\n",
    "                  num_classes=1000)\n",
    "        self.block = nn.ModuleList([\n",
    "            e.layer0,\n",
    "            e.layer1,\n",
    "            e.layer2,\n",
    "            e.layer3,\n",
    "            e.layer4,\n",
    "        ])\n",
    "        e = None\n",
    "        self.feature = nn.Conv2d(2048,32, kernel_size=1) #dummy conv for dim reduction\n",
    "        self.logit = nn.Conv2d(32,num_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size,C,H,W = x.shape\n",
    "        for i in range( len(self.block)):\n",
    "            x = self.block[i](x)\n",
    "        x = F.dropout(x,0.5,training=self.training)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = self.feature(x)\n",
    "        logit = self.logit(x)\n",
    "        return logit\n",
    "    \n",
    "def getSeresnet50Classifier():\n",
    "    model = SeResnet50Classification()\n",
    "    model=model.cuda()\n",
    "    x = torch.load(WEIGHTS+'seresnet50_classifier.pth')\n",
    "    model.load_state_dict(x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    '''Dataset for test prediction'''\n",
    "    def __init__(self, root, df, transform):\n",
    "        self.root = root\n",
    "        df['ImageId'] = df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n",
    "        self.fnames = df['ImageId'].unique().tolist()\n",
    "        self.num_samples = len(self.fnames)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.fnames[idx]\n",
    "        path = os.path.join(self.root, fname)\n",
    "        image = cv2.imread(path)\n",
    "        images = self.transform(image=image)[\"image\"]\n",
    "        return fname, images\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "sample_submission_path = '../input/severstal-steel-defect-detection/sample_submission.csv'\n",
    "test_data_folder = \"../input/severstal-steel-defect-detection/test_images\"\n",
    "df = pd.read_csv(sample_submission_path)\n",
    "mean=(0.485, 0.456, 0.406)\n",
    "std=(0.229, 0.224, 0.225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def getTransforms(tf):\n",
    "    ds = []\n",
    "    transform = albu.Compose([albu.Normalize(mean=mean,std=std),ToTensorV2()])\n",
    "    ds.append(DataLoader(\n",
    "        TestDataset(test_data_folder, df, transform),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    ))\n",
    "    for i in tf:\n",
    "        transform = albu.Compose([i,albu.Normalize(mean=mean,std=std),ToTensorV2()])\n",
    "        ds.append(DataLoader(\n",
    "            TestDataset(test_data_folder, df, transform),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        ))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "tf = [albu.HorizontalFlip(p=1)]#,albu.RandomContrast(p=1)]\n",
    "ds = getTransforms(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def sharpen(p,t=0.5):\n",
    "        if t!=0:\n",
    "            return p**t\n",
    "        else:\n",
    "            return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def classifierPredict():\n",
    "    \n",
    "    res = []\n",
    "    for _ in range(len(ds[0].dataset)*4):\n",
    "        res.append([])\n",
    "    itr = 0\n",
    "    model = Model([getBibekClassifier(),getmyClassifier(),getSeresnet50Classifier()])\n",
    "    total = len(ds[0].dataset)//batch_size\n",
    "    for loaders_batch in tqdm(zip(*ds), total=total):\n",
    "        preds = []\n",
    "        for i in range(len(model.models)):\n",
    "            preds.append([])\n",
    "        image_file = []\n",
    "        for i, (ids,features) in enumerate(loaders_batch):\n",
    "            torch.cuda.empty_cache()\n",
    "            p = model(features)\n",
    "#             preds.append(p)\n",
    "            for t in range(len(model.models)):\n",
    "                preds[t].append(p[t])\n",
    "#             if i == 1:\n",
    "#                 p = torch.flip(p,[2])\n",
    "#             if i == 2:\n",
    "#                 p = torch.flip(p,[3])\n",
    "#             if i!=1 and i!=2:\n",
    "#                 p = sharpen(p,0)\n",
    "#             else:\n",
    "#                 p = sharpen(p)\n",
    "#             preds.append(p)\n",
    "            image_file = ids\n",
    "        for t in range(len(preds)):\n",
    "            preds[t] = torch.stack(preds[t])\n",
    "            preds[t] = torch.mean(preds[t], dim=0)\n",
    "            preds[t] = torch.sigmoid(preds[t])\n",
    "        preds = torch.stack(preds)\n",
    "        preds = torch.mean(preds, dim=0)\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        \n",
    "        for p, file in zip(preds, image_file):\n",
    "            file = os.path.basename(file)\n",
    "            for i in range(4):\n",
    "                p_channel = p[i]\n",
    "                imageid_classid = file+'_'+str(i+1)\n",
    "                p_channel = (p_channel>thresholds[i]).astype(np.uint8)\n",
    "                if p_channel: \n",
    "                    res[itr] = {\n",
    "                        'ImageId_ClassId': imageid_classid,\n",
    "                        'EncodedPixels': '1 1'\n",
    "                    }\n",
    "                else:\n",
    "                    res[itr] = {\n",
    "                        'ImageId_ClassId': imageid_classid,\n",
    "                        'EncodedPixels': ''\n",
    "                    }\n",
    "                itr += 1\n",
    "\n",
    "    df = pd.DataFrame(res)\n",
    "    del model\n",
    "    del res\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113it [04:05,  2.18s/it]\n"
     ]
    }
   ],
   "source": [
    "thresholds = [0.49, 0.45, 0.45, 0.49]\n",
    "min_area = [600, 600, 1000, 2000]\n",
    "from collections import OrderedDict\n",
    "df_classification = classifierPredict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet.gluon import data, HybridBlock, nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "from mxnet.gluon.model_zoo import vision\n",
    "import glob\n",
    "from mxnet import nd as F, gluon\n",
    "from gluoncv import model_zoo as gm\n",
    "from gluoncv.model_zoo.resnetv1b import resnet50_v1s, resnet101_v1s, resnet152_v1s\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.nn import HybridBlock\n",
    "\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.nn import HybridBlock\n",
    "\n",
    "class ResNetBackbone(mx.gluon.HybridBlock):\n",
    "    def __init__(self, backbone='resnet50', pretrained_base=True,dilated=True, **kwargs):\n",
    "        super(ResNetBackbone, self).__init__()\n",
    "\n",
    "        with self.name_scope():\n",
    "            if backbone == 'resnet50':\n",
    "                pretrained = resnet50_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n",
    "            elif backbone == 'resnet101':\n",
    "                pretrained = resnet101_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n",
    "            elif backbone == 'resnet152':\n",
    "                pretrained = resnet152_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n",
    "            else:\n",
    "                raise RuntimeError(f'unknown backbone: {backbone}')\n",
    "\n",
    "            self.conv1 = pretrained.conv1\n",
    "            self.bn1 = pretrained.bn1\n",
    "            self.relu = pretrained.relu\n",
    "            self.maxpool = pretrained.maxpool\n",
    "            self.layer1 = pretrained.layer1\n",
    "            self.layer2 = pretrained.layer2\n",
    "            self.layer3 = pretrained.layer3\n",
    "            self.layer4 = pretrained.layer4\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        c1 = self.layer1(x)\n",
    "        c2 = self.layer2(c1)\n",
    "        c3 = self.layer3(c2)\n",
    "        c4 = self.layer4(c3)\n",
    "\n",
    "        return c1, c2, c3, c4\n",
    "\n",
    "class ResNetFPN(mx.gluon.HybridBlock):\n",
    "    def __init__(self, backbone= 'resnet50', backbone_lr_mult=0.1, **kwargs):\n",
    "        super(ResNetFPN, self).__init__()\n",
    "\n",
    "        self.backbone_name = backbone\n",
    "        self.backbone_lr_mult = backbone_lr_mult\n",
    "        self._kwargs = kwargs\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.backbone = ResNetBackbone(backbone=self.backbone_name, pretrained_base=False, dilated=False, **kwargs)\n",
    "\n",
    "            self.head = _FPNHead(output_channels=256, **kwargs)\n",
    "\n",
    "    def load_pretrained_weights(self):\n",
    "        pretrained = ResNetBackbone(backbone=self.backbone_name, pretrained_base=True, dilated=False, **self._kwargs)\n",
    "        backbone_params = self.backbone.collect_params()\n",
    "        pretrained_weights = pretrained.collect_params()\n",
    "        for k, v in pretrained_weights.items():\n",
    "            param_name = backbone_params.prefix + k[len(pretrained_weights.prefix):]\n",
    "            backbone_params[param_name].set_data(v.data())\n",
    "\n",
    "        self.backbone.collect_params().setattr('lr_mult', self.backbone_lr_mult)\n",
    "\n",
    "    def hybrid_forward(self,F, x):\n",
    "        c1, c2, c3, c4 = self.backbone(x)\n",
    "        p1, p2, p3, p4 = self.head(c1, c2, c3, c4)\n",
    "\n",
    "        return p1, p2, p3, p4\n",
    "\n",
    "class ResNetUnet(mx.gluon.HybridBlock):\n",
    "    def __init__(self, backbone= 'resnet50', backbone_lr_mult=0.1, cls_branch=False, **kwargs):\n",
    "        super(ResNetUnet, self).__init__()\n",
    "\n",
    "        self.backbone_name = backbone\n",
    "        self.backbone_lr_mult = backbone_lr_mult\n",
    "        self.cls_branch = cls_branch\n",
    "        self._kwargs = kwargs\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.backbone = ResNetBackbone(backbone=self.backbone_name, pretrained_base=False, dilated=False, **kwargs)\n",
    "\n",
    "            self.head = _UnetHead(**kwargs)\n",
    "\n",
    "    def load_pretrained_weights(self):\n",
    "        pretrained = ResNetBackbone(backbone=self.backbone_name, pretrained_base=True, dilated=False, **self._kwargs)\n",
    "        backbone_params = self.backbone.collect_params()\n",
    "        pretrained_weights = pretrained.collect_params()\n",
    "        for k, v in pretrained_weights.items():\n",
    "            param_name = backbone_params.prefix + k[len(pretrained_weights.prefix):]\n",
    "            backbone_params[param_name].set_data(v.data())\n",
    "\n",
    "        self.backbone.collect_params().setattr('lr_mult', self.backbone_lr_mult)\n",
    "\n",
    "    def hybrid_forward(self,F, x):\n",
    "        c1, c2, c3, c4 = self.backbone(x)\n",
    "        out = self.head(c1, c2, c3, c4)\n",
    "        if self.cls_branch:\n",
    "            logits = F.max(F.mean(out, axis=1), axis=(1, 2))\n",
    "            return out, logits\n",
    "        return out\n",
    "\n",
    "class _DecoderBlock(HybridBlock):\n",
    "    def __init__(self, output_channels, norm_layer=nn.BatchNorm):\n",
    "        super(_DecoderBlock, self).__init__()\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.block = nn.HybridSequential()\n",
    "            self.block.add(ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer))\n",
    "            self.block.add(ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer))\n",
    "\n",
    "    def hybrid_forward(self, F, x, y=None):\n",
    "\n",
    "        if y is not None:\n",
    "            x = F.contrib.BilinearResize2D(x, scale_height=2, scale_width=2)\n",
    "            x = F.concat(x, y, dim=1)\n",
    "        out = self.block(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class _UnetHead(HybridBlock):\n",
    "    def __init__(self, num_classes, output_channels=[256, 128, 64, 32], scale=4, norm_layer=nn.BatchNorm):\n",
    "        super(_UnetHead, self).__init__()\n",
    "        \n",
    "        self.scale = scale\n",
    "        with self.name_scope():\n",
    "            self.block4 = _DecoderBlock(output_channels[0], norm_layer=norm_layer)\n",
    "            self.block3 = _DecoderBlock(output_channels[1], norm_layer=norm_layer)\n",
    "            self.block2 = _DecoderBlock(output_channels[2], norm_layer=norm_layer)\n",
    "            self.block1 = _DecoderBlock(output_channels[3], norm_layer=norm_layer)\n",
    "            self.postprocess_block = nn.Conv2D(num_classes, kernel_size=1)\n",
    "\n",
    "    def hybrid_forward(self, F, c1, c2, c3, c4):\n",
    "\n",
    "        p4 = self.block4(c4)\n",
    "        p3 = self.block3(p4, c3)\n",
    "        p2 = self.block2(p3, c2)\n",
    "        p1 = self.block1(p2, c1)\n",
    "        if self.scale > 1:\n",
    "            p1 = F.contrib.BilinearResize2D(p1, scale_height=self.scale, scale_width=self.scale)\n",
    "        out = self.postprocess_block(p1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class _FPNHead(HybridBlock):\n",
    "    def __init__(self, output_channels=256, norm_layer=nn.BatchNorm):\n",
    "        super(_FPNHead, self).__init__()\n",
    "        self._hdsize = {}\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.block4 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n",
    "            self.block3 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n",
    "            self.block2 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n",
    "            self.block1 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n",
    "\n",
    "    def hybrid_forward(self, F, c1, c2, c3, c4):\n",
    "        p4 = self.block4(c4)\n",
    "        p3 = self._resize_as(F, 'id_1', p4, c3) + self.block3(c3)\n",
    "        p2 = self._resize_as(F, 'id_2', p3, c2) + self.block2(c2)\n",
    "        p1 = self._resize_as(F, 'id_3', p2, c1) + self.block1(c1)\n",
    "\n",
    "        return p1, p2, p3, p4\n",
    "\n",
    "    def _resize_as(self, F, name, x, y):\n",
    "        h_key = name + '_h'\n",
    "        w_key = name + '_w'\n",
    "\n",
    "        if hasattr(y, 'shape'):\n",
    "            _, _, h, w = y.shape\n",
    "            _, _, h2, w2 = x.shape\n",
    "\n",
    "            if h == h2 and w == w2:\n",
    "                h = 0\n",
    "                w = 0\n",
    "\n",
    "            self._hdsize[h_key] = h\n",
    "            self._hdsize[w_key] = w\n",
    "        else:\n",
    "            h, w = self._hdsize[h_key], self._hdsize[w_key]\n",
    "\n",
    "        if h == 0 and w == 0:\n",
    "            return x\n",
    "        else:\n",
    "            return F.contrib.BilinearResize2D(x, height=h, width=w)\n",
    "\n",
    "\n",
    "class SemanticFPNHead(HybridBlock):\n",
    "    def __init__(self, num_classes, output_channels=128, norm_layer=nn.BatchNorm):\n",
    "        super(SemanticFPNHead, self).__init__()\n",
    "        self._hdsize = {}\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.block4_1 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n",
    "            self.block4_2 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n",
    "            self.block4_3 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n",
    "\n",
    "            self.block3_1 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n",
    "            self.block3_2 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n",
    "\n",
    "            self.block2 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n",
    "            self.block1 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n",
    "\n",
    "            self.postprocess_block = nn.Conv2D(num_classes, kernel_size=1)\n",
    "\n",
    "    def hybrid_forward(self, F, c1, c2, c3, c4):\n",
    "        out4 = self._resize_as(F, 'id_1', self.block4_1(c4), c3)\n",
    "        out4 = self._resize_as(F, 'id_2', self.block4_2(out4), c2)\n",
    "        out4 = self._resize_as(F, 'id_3', self.block4_3(out4), c1)\n",
    "\n",
    "        out3 = self._resize_as(F, 'id_4', self.block3_1(c3), c2)\n",
    "        out3 = self._resize_as(F, 'id_5', self.block3_2(out3), c1)\n",
    "\n",
    "        out2 = self._resize_as(F, 'id_6', self.block2(c2), c1)\n",
    "\n",
    "        out1 = self.block1(c1)\n",
    "\n",
    "        out = out1 + out2 + out3 + out4\n",
    "\n",
    "        out = self.postprocess_block(out)\n",
    "        out = F.contrib.BilinearResize2D(out,scale_height=4,scale_width=4)\n",
    "        return out\n",
    "\n",
    "    def _resize_as(self, F,name, x, y):\n",
    "        h_key = name + '_h'\n",
    "        w_key = name + '_w'\n",
    "\n",
    "        if hasattr(y, 'shape'):\n",
    "            _, _, h, w = y.shape\n",
    "            _, _, h2, w2 = x.shape\n",
    "\n",
    "            if h == h2 and w == w2:\n",
    "                h = 0\n",
    "                w = 0\n",
    "\n",
    "            self._hdsize[h_key]=h\n",
    "            self._hdsize[w_key]=w\n",
    "        else:\n",
    "            h, w = self._hdsize[h_key], self._hdsize[w_key]\n",
    "\n",
    "        if h == 0 and w == 0:\n",
    "            return x\n",
    "        else:\n",
    "            return F.contrib.BilinearResize2D(x,height=h,width=w)\n",
    "\n",
    "\n",
    "class ConvBlock(HybridBlock):\n",
    "    def __init__(self, output_channels, kernel_size, padding=0, activation='relu', norm_layer=nn.BatchNorm):\n",
    "        super().__init__()\n",
    "        self.body = nn.HybridSequential()\n",
    "        self.body.add(\n",
    "            nn.Conv2D(output_channels, kernel_size=kernel_size, padding=padding, activation=activation),\n",
    "            norm_layer(in_channels=output_channels)\n",
    "        )\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return self.body(x)\n",
    "\n",
    "class SteelUnet(HybridBlock):\n",
    "    \n",
    "    def __init__(self, n_classes=5, ctx=mx.cpu()):\n",
    "        super().__init__()\n",
    "        with self.name_scope():\n",
    "            self.feature_extractor = ResNetFPN()\n",
    "            self.segment_head = SemanticFPNHead(num_classes=n_classes)\n",
    "    def hybrid_forward(self, F, x):\n",
    "        fpn_feature = self.feature_extractor(x)\n",
    "        segment_out = self.segment_head(*fpn_feature)\n",
    "        return segment_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.gpu()\n",
    "unet = ResNetUnet(output_channels=[256, 128, 64, 32], num_classes=5)\n",
    "unet.load_parameters('../input/mxnetweights/unet_34_-1.params')\n",
    "unet.collect_params().reset_ctx(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def remove_small_one(predict, min_size):\n",
    "    H,W = predict.shape\n",
    "    num_component, component = cv2.connectedComponents(predict.astype(np.uint8))\n",
    "    predict = np.zeros((H,W), np.bool)\n",
    "    for c in range(1,num_component):\n",
    "        p = (component==c)\n",
    "        if p.sum()>min_size:\n",
    "            predict[p] = True\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# import time\n",
    "# test_stage\n",
    "trans = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=(0.485, 0.456, 0.406),\n",
    "                   std=(0.229, 0.224, 0.225)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "# test_dir = \"../input/severstal-steel-defect-detection/test_images/\"\n",
    "\n",
    "# imglists = glob.glob(test_dir + '/*g')\n",
    "# oriims = []\n",
    "# preds = []\n",
    "# random.shuffle(imglists)\n",
    "# ImageId_ClassIds = []\n",
    "# EncodedPixels = []\n",
    "\n",
    "augs = ['flip_lr', 'flip_ud']\n",
    "# thresholds = [0.5, 0.5, 0.5, 0.5]\n",
    "# min_area = [600, 600, 1000, 2000]\n",
    "# min_area = [1, 1, 1, 1]\n",
    "# t1 = time.time()\n",
    "# for i, item in enumerate(tqdm(imglists)):\n",
    "#     timg = cv2.imread(item)[:, :, ::-1]\n",
    "#     img = mx.nd.array([timg])\n",
    "#     input_img = trans(img)\n",
    "#     num_aug = 0\n",
    "    \n",
    "#     if 1:\n",
    "#         out = unet(input_img.as_in_context(ctx))\n",
    "        \n",
    "#         out = F.softmax(out, axis=1)\n",
    "#         out = F.where(out > 0.5, out, F.zeros_like(out))\n",
    "#         pred_inds = F.argmax(out, axis=1)\n",
    "#         oriims.append(timg)\n",
    "#         preds.append(pred_inds)\n",
    "#         out_mask = sharpen(out, 0)\n",
    "        \n",
    "#         num_aug += 1\n",
    "#     if 'flip_lr' in augs:\n",
    "#         input_img_lr = F.flip(input_img, axis=3)\n",
    "#         out = unet(input_img_lr.as_in_context(ctx))\n",
    "#         out = F.softmax(out, axis=1)\n",
    "#         out = F.where(out > 0.5, out, F.zeros_like(out))\n",
    "#         out_mask += sharpen(F.flip(out, axis=3))\n",
    "#         num_aug += 1\n",
    "    \n",
    "#     out_mask = out_mask * 1.0 / num_aug\n",
    "#     out = out_mask[:, 1:, :, :].asnumpy()\n",
    "#     ImageId = item.split('/')[-1]\n",
    "#     pred_inds = pred_inds.asnumpy()\n",
    "#     for j in range(4):\n",
    "#         Id = ImageId + '_'+str(j+1)\n",
    "#         tmp_mask = np.where(out[:, j, :, :] > thresholds[j], 1.0, 0)\n",
    "# #         tmp_mask = np.where(pred_inds[0, :, :]==(j+1), 1, 0)\n",
    "#         if np.sum(tmp_mask) < min_area[j]:\n",
    "#             tmp_mask = np.zeros_like(tmp_mask)\n",
    "#         if np.sum(tmp_mask) < 10:\n",
    "#             EncodedPixel = ''\n",
    "#         else:\n",
    "#             EncodedPixel = mask2rle(tmp_mask)\n",
    "\n",
    "#         ImageId_ClassIds.append(Id)\n",
    "#         EncodedPixels.append(EncodedPixel)\n",
    "# dur = time.time() - t1\n",
    "# print(\"cost time:{}\".format(dur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# submission =  pd.read_csv(\"../input/severstal-steel-defect-detection/sample_submission.csv\")\n",
    "# print(len(ImageId),len(submission['ImageId_ClassId']))\n",
    "# # len(set(submission['ImageId_ClassId'])-set(Ids))\n",
    "# # assert set(Ids) == set(submission['ImageId_ClassId'])\n",
    "\n",
    "# for i, encoded in zip(ImageId_ClassIds,EncodedPixels):\n",
    "#     submission.loc[submission['ImageId_ClassId']==i,[\"EncodedPixels\"]] =  encoded\n",
    "\n",
    "# submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMXPred(item):\n",
    "    timg = cv2.imread(item)[:, :, ::-1]\n",
    "    img = mx.nd.array([timg])\n",
    "    input_img = trans(img)\n",
    "#     num_aug = 0\n",
    "    \n",
    "    if 1:\n",
    "        out = unet(input_img.as_in_context(ctx))\n",
    "        out = F.softmax(out, axis=1)\n",
    "        out = F.where(out > 0.5, out, F.zeros_like(out))\n",
    "#         pred_inds = F.argmax(out, axis=1)\n",
    "        out_mask = sharpen(out, 0)\n",
    "#         num_aug += 1\n",
    "        \n",
    "#     if 'flip_lr' in augs:\n",
    "#     if i == 1:\n",
    "#         input_img_lr = F.flip(input_img, axis=3)\n",
    "#         out = unet(input_img.as_in_context(ctx))\n",
    "#         out = F.flip(out, axis=3)\n",
    "#         out = F.softmax(out, axis=1)\n",
    "#         out = F.where(out > 0.5, out, F.zeros_like(out))\n",
    "#         out_mask = sharpen(F.flip(out, axis=3))\n",
    "#         num_aug += 1\n",
    "    \n",
    "#     out = out * 1.0 / 2\n",
    "    out = out_mask[:, 1:, :, :].asnumpy()\n",
    "    return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask2rle(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels= img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6271aac134e4e3482bb1239de9ace72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=112), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "thresholds = [0.45, 0.49, 0.45, 0.49]\n",
    "min_area = [600, 600, 1000, 2000]\n",
    "\n",
    "res = []\n",
    "# Iterate over all TTA loaders\n",
    "total = len(datasets[0])//batch_size\n",
    "for loaders_batch in tqdm_notebook(zip(*loaders), total=total):\n",
    "    preds = []\n",
    "    for m in range(len(model.models)):\n",
    "        preds.append([])\n",
    "    image_file = []\n",
    "    \n",
    "    for i, batch in enumerate(loaders_batch):\n",
    "        image_file = batch['image_file']\n",
    "        features = batch['features'].cuda()\n",
    "        x = model(features)\n",
    "        for t,v in enumerate(x):\n",
    "            p = datasets[i].inverse(v)\n",
    "            preds[t].append(p)\n",
    "#         if i == 0:\n",
    "#             p = sharpen(p,0)\n",
    "#         else:\n",
    "#             p = sharpen(p,0.5)\n",
    "        # inverse operations for TTA\n",
    "#         p = p.detach().cpu().numpy()\n",
    "#         preds.append(p)\n",
    "    # TTA mean\n",
    "    for t in range(len(model.models)):\n",
    "        preds[t] = torch.stack(preds[t])\n",
    "        preds[t] = torch.mean(preds[t],dim=0)\n",
    "        preds[t] = torch.sigmoid(preds[t])\n",
    "        \n",
    "    preds = torch.stack(preds)\n",
    "    preds = torch.mean(preds, dim=0)\n",
    "    x = []\n",
    "    for f in image_file:\n",
    "        out = getMXPred(f)\n",
    "        x.append(out)\n",
    "    x = np.concatenate(x)\n",
    "    x = torch.FloatTensor(x).to('cuda')\n",
    "        \n",
    "    preds = torch.stack([preds,x])\n",
    "    preds = torch.mean(preds, dim=0)\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    # Batch post processing\n",
    "    for p, file in zip(preds, image_file):\n",
    "        file = os.path.basename(file)\n",
    "        # Image postprocessing\n",
    "        for i in range(4):\n",
    "            p_channel = p[i]\n",
    "            imageid_classid = file+'_'+str(i+1)\n",
    "            p_channel = (p_channel>thresholds[i]).astype(np.uint8)\n",
    "            if p_channel.sum() < min_area[i]:\n",
    "                p_channel = np.zeros(p_channel.shape, dtype=p_channel.dtype)\n",
    "#             if np.sum(p_channel) < 10:\n",
    "#                 res.append({\n",
    "#                     'ImageId_ClassId': imageid_classid,\n",
    "#                     'EncodedPixels': ''\n",
    "#                 })\n",
    "#             else:\n",
    "            tp = df_classification[df_classification['ImageId_ClassId']==imageid_classid]['EncodedPixels'].values\n",
    "            mask = mask2rle(p_channel)\n",
    "            if tp[0] == '1 1' and mask != '': \n",
    "#                 if mask !='':\n",
    "                res.append({\n",
    "                    'ImageId_ClassId': imageid_classid,\n",
    "                    'EncodedPixels': mask\n",
    "                })\n",
    "            else:\n",
    "                res.append({\n",
    "                    'ImageId_ClassId': imageid_classid,\n",
    "                    'EncodedPixels': ''\n",
    "                })\n",
    "                \n",
    "df = pd.DataFrame(res)\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getMXPred(item,i):\n",
    "#     timg = cv2.imread(item)[:, :, ::-1]\n",
    "#     img = mx.nd.array([timg])\n",
    "#     input_img = trans(img)\n",
    "#     num_aug = 0\n",
    "    \n",
    "#     if i==0:\n",
    "#         out = unet(input_img.as_in_context(ctx))\n",
    "# #         out = F.softmax(out, axis=1)\n",
    "# #         out = F.where(out > 0.5, out, F.zeros_like(out))\n",
    "# #         pred_inds = F.argmax(out, axis=1)\n",
    "# #         out_mask = sharpen(out, 0)\n",
    "# #         num_aug += 1\n",
    "        \n",
    "#     if i==1:\n",
    "#         input_img_lr = F.flip(input_img, axis=3)\n",
    "#         out = unet(input_img_lr.as_in_context(ctx))\n",
    "#         out = F.flip(out, axis=3)\n",
    "# #         print(1)\n",
    "# #         out = F.softmax(out, axis=1)\n",
    "# #         out = F.where(out > 0.5, out, F.zeros_like(out))\n",
    "# #         out_mask += sharpen(F.flip(out, axis=3))\n",
    "# #         num_aug += 1\n",
    "    \n",
    "# #     out_mask = out_mask * 1.0 / num_aug\n",
    "#     out = out[:, 1:, :, :].asnumpy()\n",
    "#     return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = []\n",
    "# # Iterate over all TTA loaders\n",
    "# total = len(datasets[0])//batch_size\n",
    "# for loaders_batch in tqdm_notebook(zip(*loaders), total=total):\n",
    "#     preds = []\n",
    "#     image_file = []\n",
    "#     for i, batch in enumerate(loaders_batch):\n",
    "#         image_file = batch['image_file']\n",
    "#         features = batch['features'].cuda()\n",
    "#         resx = []\n",
    "#         x = []\n",
    "#         for f in image_file:\n",
    "#             out = getMXPred(f,i)\n",
    "#             if i == 0:\n",
    "#                 out = sharpen(out,0)\n",
    "#             else:\n",
    "#                 out = sharpen(out,0.5)\n",
    "#             x.append(out)\n",
    "#         x = np.concatenate(x)\n",
    "#         x = torch.FloatTensor(x).to('cuda')\n",
    "#         x = torch.sigmoid(x)\n",
    "#         resx.append(x)\n",
    "#         with torch.no_grad():\n",
    "#             for m in [unet_se_resnext50_32x4d, unet_mobilenet2, unet_resnet34]:\n",
    "#                 x = m(features)\n",
    "#                 x = datasets[i].inverse(x)\n",
    "#                 x = torch.sigmoid(x)\n",
    "                \n",
    "#                 if i == 0:\n",
    "#                     x = sharpen(x,0)\n",
    "#                 else:\n",
    "#                     x = sharpen(x,0.5)\n",
    "#                 resx.append(x)\n",
    "        \n",
    "            \n",
    "#         resx = torch.stack(resx)\n",
    "#         resx = torch.mean(resx, dim=0)\n",
    "# #         resx = torch.sigmoid(resx)\n",
    "        \n",
    "#         # inverse operations for TTA\n",
    "# #         p = p.detach().cpu().numpy()\n",
    "#         preds.append(resx)\n",
    "#     # TTA mean\n",
    "#     preds = torch.stack(preds)\n",
    "#     preds = torch.mean(preds, dim=0)\n",
    "#     preds = preds.detach().cpu().numpy()\n",
    "#     # Batch post processing\n",
    "#     for p, file in zip(preds, image_file):\n",
    "#         file = os.path.basename(file)\n",
    "#         # Image postprocessing\n",
    "#         for i in range(4):\n",
    "#             p_channel = p[i]\n",
    "#             imageid_classid = file+'_'+str(i+1)\n",
    "#             p_channel = (p_channel>thresholds[i]).astype(np.uint8)\n",
    "#             if p_channel.sum() < min_area[i]:\n",
    "#                 p_channel = np.zeros(p_channel.shape, dtype=p_channel.dtype)\n",
    "# #             if np.sum(p_channel) < 10:\n",
    "# #                 res.append({\n",
    "# #                     'ImageId_ClassId': imageid_classid,\n",
    "# #                     'EncodedPixels': ''\n",
    "# #                 })\n",
    "# #             else:\n",
    "#             tp = df_classification[df_classification['ImageId_ClassId']==imageid_classid]['EncodedPixels'].values\n",
    "#             mask = mask2rle(p_channel)\n",
    "#             if tp[0] == '1 1' and mask != '': \n",
    "# #                 if mask !='':\n",
    "#                 res.append({\n",
    "#                     'ImageId_ClassId': imageid_classid,\n",
    "#                     'EncodedPixels': mask\n",
    "#                 })\n",
    "#             else:\n",
    "#                 res.append({\n",
    "#                     'ImageId_ClassId': imageid_classid,\n",
    "#                     'EncodedPixels': ''\n",
    "#                 })\n",
    "                \n",
    "# df = pd.DataFrame(res)\n",
    "# df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(res)\n",
    "# df = df.fillna('')\n",
    "# df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_submission_csv(df):\n",
    "    text = ''\n",
    "    df['Class'] = df['ImageId_ClassId'].str[-1].astype(np.int32)\n",
    "    df['Label'] = (df['EncodedPixels']!='').astype(np.int32)\n",
    "    num_image = len(df)//4\n",
    "    num = len(df)\n",
    "\n",
    "    pos = (df['Label']==1).sum()\n",
    "    neg = num-pos\n",
    "\n",
    "\n",
    "    pos1 = ((df['Class']==1) & (df['Label']==1)).sum()\n",
    "    pos2 = ((df['Class']==2) & (df['Label']==1)).sum()\n",
    "    pos3 = ((df['Class']==3) & (df['Label']==1)).sum()\n",
    "    pos4 = ((df['Class']==4) & (df['Label']==1)).sum()\n",
    "\n",
    "    neg1 = num_image-pos1\n",
    "    neg2 = num_image-pos2\n",
    "    neg3 = num_image-pos3\n",
    "    neg4 = num_image-pos4\n",
    "\n",
    "\n",
    "    text += 'compare with LB probing ... \\n'\n",
    "    text += '\\t\\tnum_image = %5d(1801) \\n'%num_image\n",
    "    text += '\\t\\tnum  = %5d(7204) \\n'%num\n",
    "    text += '\\n'\n",
    "\n",
    "    text += '\\t\\tpos1 = %5d( 128)  %0.3f\\n'%(pos1,pos1/128)\n",
    "    text += '\\t\\tpos2 = %5d(  43)  %0.3f\\n'%(pos2,pos2/43)\n",
    "    text += '\\t\\tpos3 = %5d( 741)  %0.3f\\n'%(pos3,pos3/741)\n",
    "    text += '\\t\\tpos4 = %5d( 120)  %0.3f\\n'%(pos4,pos4/120)\n",
    "    text += '\\n'\n",
    "\n",
    "    text += '\\t\\tneg1 = %5d(1673)  %0.3f  %3d\\n'%(neg1,neg1/1673, neg1-1673)\n",
    "    text += '\\t\\tneg2 = %5d(1758)  %0.3f  %3d\\n'%(neg2,neg2/1758, neg2-1758)\n",
    "    text += '\\t\\tneg3 = %5d(1060)  %0.3f  %3d\\n'%(neg3,neg3/1060, neg3-1060)\n",
    "    text += '\\t\\tneg4 = %5d(1681)  %0.3f  %3d\\n'%(neg4,neg4/1681, neg4-1681)\n",
    "    text += '--------------------------------------------------\\n'\n",
    "    text += '\\t\\tneg  = %5d(6172)  %0.3f  %3d \\n'%(neg,neg/6172, neg-6172)\n",
    "    text += '\\n'\n",
    "\n",
    "#     if 1:\n",
    "#     #compare with reference\n",
    "#         pass\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare with LB probing ... \n",
      "\t\tnum_image =  1801(1801) \n",
      "\t\tnum  =  7204(7204) \n",
      "\n",
      "\t\tpos1 =   123( 128)  0.961\n",
      "\t\tpos2 =    14(  43)  0.326\n",
      "\t\tpos3 =   644( 741)  0.869\n",
      "\t\tpos4 =   116( 120)  0.967\n",
      "\n",
      "\t\tneg1 =  1678(1673)  1.003    5\n",
      "\t\tneg2 =  1787(1758)  1.016   29\n",
      "\t\tneg3 =  1157(1060)  1.092   97\n",
      "\t\tneg4 =  1685(1681)  1.002    4\n",
      "--------------------------------------------------\n",
      "\t\tneg  =  6307(6172)  1.022  135 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = summarise_submission_csv(df_classification)\n",
    "print(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "29193362ad9e40019dab8a6b21252d18": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_45e6b98182c942e380c18bd05736ff3b",
       "placeholder": "​",
       "style": "IPY_MODEL_c64e77c00a2e49d5b0049360e14dc6e1",
       "value": " 113/? [07:09&lt;00:00,  3.80s/it]"
      }
     },
     "45e6b98182c942e380c18bd05736ff3b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6bcc7df9f3064394a11f43e24ad0f6ac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d3a9052ffbd432e937c4b3786235101": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "b6271aac134e4e3482bb1239de9ace72": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fd8f404a6de44a379d1c8c81d9cefc10",
        "IPY_MODEL_29193362ad9e40019dab8a6b21252d18"
       ],
       "layout": "IPY_MODEL_f6f70a98af96470890a1dcf54cc03214"
      }
     },
     "c64e77c00a2e49d5b0049360e14dc6e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f6f70a98af96470890a1dcf54cc03214": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fd8f404a6de44a379d1c8c81d9cefc10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6bcc7df9f3064394a11f43e24ad0f6ac",
       "max": 112,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8d3a9052ffbd432e937c4b3786235101",
       "value": 112
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
