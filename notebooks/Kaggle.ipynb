{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from tqdm import tqdm_notebook,tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.jit import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet.gluon import data, HybridBlock\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "from mxnet.gluon.model_zoo import vision\n",
    "import glob\n",
    "from mxnet import nd as gluon\n",
    "from gluoncv import model_zoo as gm\n",
    "from gluoncv.model_zoo.resnetv1b import resnet50_v1s, resnet101_v1s, resnet152_v1s\n",
    "from mxnet.gluon import nn as nn_mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_segmentation = []\n",
    "model_classification = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlcomp trained models\n",
    "def getmlcompModels():\n",
    "    unet_se_resnext50_32x4d = \\\n",
    "        load('output_weights/unet_se_resnext50_32x4d.pth').cuda()\n",
    "    unet_mobilenet2 = load('output_weights/unet_mobilenet2.pth').cuda()\n",
    "    unet_resnet34 = load('output_weights/unet_resnet34.pth').cuda()\n",
    "    return [unet_se_resnext50_32x4d, unet_mobilenet2, unet_resnet34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes from Heng's baseline\n",
    "# This code is for classifcation model\n",
    "\n",
    "BatchNorm2d = nn.BatchNorm2d\n",
    "\n",
    "IMAGE_RGB_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGE_RGB_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "CONVERSION=[\n",
    " 'block0.0.weight',\t(64, 3, 7, 7),\t 'conv1.weight',\t(64, 3, 7, 7),\n",
    " 'block0.1.weight',\t(64,),\t 'bn1.weight',\t(64,),\n",
    " 'block0.1.bias',\t(64,),\t 'bn1.bias',\t(64,),\n",
    " 'block0.1.running_mean',\t(64,),\t 'bn1.running_mean',\t(64,),\n",
    " 'block0.1.running_var',\t(64,),\t 'bn1.running_var',\t(64,),\n",
    " 'block1.1.conv_bn1.conv.weight',\t(64, 64, 3, 3),\t 'layer1.0.conv1.weight',\t(64, 64, 3, 3),\n",
    " 'block1.1.conv_bn1.bn.weight',\t(64,),\t 'layer1.0.bn1.weight',\t(64,),\n",
    " 'block1.1.conv_bn1.bn.bias',\t(64,),\t 'layer1.0.bn1.bias',\t(64,),\n",
    " 'block1.1.conv_bn1.bn.running_mean',\t(64,),\t 'layer1.0.bn1.running_mean',\t(64,),\n",
    " 'block1.1.conv_bn1.bn.running_var',\t(64,),\t 'layer1.0.bn1.running_var',\t(64,),\n",
    " 'block1.1.conv_bn2.conv.weight',\t(64, 64, 3, 3),\t 'layer1.0.conv2.weight',\t(64, 64, 3, 3),\n",
    " 'block1.1.conv_bn2.bn.weight',\t(64,),\t 'layer1.0.bn2.weight',\t(64,),\n",
    " 'block1.1.conv_bn2.bn.bias',\t(64,),\t 'layer1.0.bn2.bias',\t(64,),\n",
    " 'block1.1.conv_bn2.bn.running_mean',\t(64,),\t 'layer1.0.bn2.running_mean',\t(64,),\n",
    " 'block1.1.conv_bn2.bn.running_var',\t(64,),\t 'layer1.0.bn2.running_var',\t(64,),\n",
    " 'block1.2.conv_bn1.conv.weight',\t(64, 64, 3, 3),\t 'layer1.1.conv1.weight',\t(64, 64, 3, 3),\n",
    " 'block1.2.conv_bn1.bn.weight',\t(64,),\t 'layer1.1.bn1.weight',\t(64,),\n",
    " 'block1.2.conv_bn1.bn.bias',\t(64,),\t 'layer1.1.bn1.bias',\t(64,),\n",
    " 'block1.2.conv_bn1.bn.running_mean',\t(64,),\t 'layer1.1.bn1.running_mean',\t(64,),\n",
    " 'block1.2.conv_bn1.bn.running_var',\t(64,),\t 'layer1.1.bn1.running_var',\t(64,),\n",
    " 'block1.2.conv_bn2.conv.weight',\t(64, 64, 3, 3),\t 'layer1.1.conv2.weight',\t(64, 64, 3, 3),\n",
    " 'block1.2.conv_bn2.bn.weight',\t(64,),\t 'layer1.1.bn2.weight',\t(64,),\n",
    " 'block1.2.conv_bn2.bn.bias',\t(64,),\t 'layer1.1.bn2.bias',\t(64,),\n",
    " 'block1.2.conv_bn2.bn.running_mean',\t(64,),\t 'layer1.1.bn2.running_mean',\t(64,),\n",
    " 'block1.2.conv_bn2.bn.running_var',\t(64,),\t 'layer1.1.bn2.running_var',\t(64,),\n",
    " 'block1.3.conv_bn1.conv.weight',\t(64, 64, 3, 3),\t 'layer1.2.conv1.weight',\t(64, 64, 3, 3),\n",
    " 'block1.3.conv_bn1.bn.weight',\t(64,),\t 'layer1.2.bn1.weight',\t(64,),\n",
    " 'block1.3.conv_bn1.bn.bias',\t(64,),\t 'layer1.2.bn1.bias',\t(64,),\n",
    " 'block1.3.conv_bn1.bn.running_mean',\t(64,),\t 'layer1.2.bn1.running_mean',\t(64,),\n",
    " 'block1.3.conv_bn1.bn.running_var',\t(64,),\t 'layer1.2.bn1.running_var',\t(64,),\n",
    " 'block1.3.conv_bn2.conv.weight',\t(64, 64, 3, 3),\t 'layer1.2.conv2.weight',\t(64, 64, 3, 3),\n",
    " 'block1.3.conv_bn2.bn.weight',\t(64,),\t 'layer1.2.bn2.weight',\t(64,),\n",
    " 'block1.3.conv_bn2.bn.bias',\t(64,),\t 'layer1.2.bn2.bias',\t(64,),\n",
    " 'block1.3.conv_bn2.bn.running_mean',\t(64,),\t 'layer1.2.bn2.running_mean',\t(64,),\n",
    " 'block1.3.conv_bn2.bn.running_var',\t(64,),\t 'layer1.2.bn2.running_var',\t(64,),\n",
    " 'block2.0.conv_bn1.conv.weight',\t(128, 64, 3, 3),\t 'layer2.0.conv1.weight',\t(128, 64, 3, 3),\n",
    " 'block2.0.conv_bn1.bn.weight',\t(128,),\t 'layer2.0.bn1.weight',\t(128,),\n",
    " 'block2.0.conv_bn1.bn.bias',\t(128,),\t 'layer2.0.bn1.bias',\t(128,),\n",
    " 'block2.0.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.0.bn1.running_mean',\t(128,),\n",
    " 'block2.0.conv_bn1.bn.running_var',\t(128,),\t 'layer2.0.bn1.running_var',\t(128,),\n",
    " 'block2.0.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.0.conv2.weight',\t(128, 128, 3, 3),\n",
    " 'block2.0.conv_bn2.bn.weight',\t(128,),\t 'layer2.0.bn2.weight',\t(128,),\n",
    " 'block2.0.conv_bn2.bn.bias',\t(128,),\t 'layer2.0.bn2.bias',\t(128,),\n",
    " 'block2.0.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.0.bn2.running_mean',\t(128,),\n",
    " 'block2.0.conv_bn2.bn.running_var',\t(128,),\t 'layer2.0.bn2.running_var',\t(128,),\n",
    " 'block2.0.shortcut.conv.weight',\t(128, 64, 1, 1),\t 'layer2.0.downsample.0.weight',\t(128, 64, 1, 1),\n",
    " 'block2.0.shortcut.bn.weight',\t(128,),\t 'layer2.0.downsample.1.weight',\t(128,),\n",
    " 'block2.0.shortcut.bn.bias',\t(128,),\t 'layer2.0.downsample.1.bias',\t(128,),\n",
    " 'block2.0.shortcut.bn.running_mean',\t(128,),\t 'layer2.0.downsample.1.running_mean',\t(128,),\n",
    " 'block2.0.shortcut.bn.running_var',\t(128,),\t 'layer2.0.downsample.1.running_var',\t(128,),\n",
    " 'block2.1.conv_bn1.conv.weight',\t(128, 128, 3, 3),\t 'layer2.1.conv1.weight',\t(128, 128, 3, 3),\n",
    " 'block2.1.conv_bn1.bn.weight',\t(128,),\t 'layer2.1.bn1.weight',\t(128,),\n",
    " 'block2.1.conv_bn1.bn.bias',\t(128,),\t 'layer2.1.bn1.bias',\t(128,),\n",
    " 'block2.1.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.1.bn1.running_mean',\t(128,),\n",
    " 'block2.1.conv_bn1.bn.running_var',\t(128,),\t 'layer2.1.bn1.running_var',\t(128,),\n",
    " 'block2.1.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.1.conv2.weight',\t(128, 128, 3, 3),\n",
    " 'block2.1.conv_bn2.bn.weight',\t(128,),\t 'layer2.1.bn2.weight',\t(128,),\n",
    " 'block2.1.conv_bn2.bn.bias',\t(128,),\t 'layer2.1.bn2.bias',\t(128,),\n",
    " 'block2.1.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.1.bn2.running_mean',\t(128,),\n",
    " 'block2.1.conv_bn2.bn.running_var',\t(128,),\t 'layer2.1.bn2.running_var',\t(128,),\n",
    " 'block2.2.conv_bn1.conv.weight',\t(128, 128, 3, 3),\t 'layer2.2.conv1.weight',\t(128, 128, 3, 3),\n",
    " 'block2.2.conv_bn1.bn.weight',\t(128,),\t 'layer2.2.bn1.weight',\t(128,),\n",
    " 'block2.2.conv_bn1.bn.bias',\t(128,),\t 'layer2.2.bn1.bias',\t(128,),\n",
    " 'block2.2.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.2.bn1.running_mean',\t(128,),\n",
    " 'block2.2.conv_bn1.bn.running_var',\t(128,),\t 'layer2.2.bn1.running_var',\t(128,),\n",
    " 'block2.2.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.2.conv2.weight',\t(128, 128, 3, 3),\n",
    " 'block2.2.conv_bn2.bn.weight',\t(128,),\t 'layer2.2.bn2.weight',\t(128,),\n",
    " 'block2.2.conv_bn2.bn.bias',\t(128,),\t 'layer2.2.bn2.bias',\t(128,),\n",
    " 'block2.2.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.2.bn2.running_mean',\t(128,),\n",
    " 'block2.2.conv_bn2.bn.running_var',\t(128,),\t 'layer2.2.bn2.running_var',\t(128,),\n",
    " 'block2.3.conv_bn1.conv.weight',\t(128, 128, 3, 3),\t 'layer2.3.conv1.weight',\t(128, 128, 3, 3),\n",
    " 'block2.3.conv_bn1.bn.weight',\t(128,),\t 'layer2.3.bn1.weight',\t(128,),\n",
    " 'block2.3.conv_bn1.bn.bias',\t(128,),\t 'layer2.3.bn1.bias',\t(128,),\n",
    " 'block2.3.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.3.bn1.running_mean',\t(128,),\n",
    " 'block2.3.conv_bn1.bn.running_var',\t(128,),\t 'layer2.3.bn1.running_var',\t(128,),\n",
    " 'block2.3.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.3.conv2.weight',\t(128, 128, 3, 3),\n",
    " 'block2.3.conv_bn2.bn.weight',\t(128,),\t 'layer2.3.bn2.weight',\t(128,),\n",
    " 'block2.3.conv_bn2.bn.bias',\t(128,),\t 'layer2.3.bn2.bias',\t(128,),\n",
    " 'block2.3.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.3.bn2.running_mean',\t(128,),\n",
    " 'block2.3.conv_bn2.bn.running_var',\t(128,),\t 'layer2.3.bn2.running_var',\t(128,),\n",
    " 'block3.0.conv_bn1.conv.weight',\t(256, 128, 3, 3),\t 'layer3.0.conv1.weight',\t(256, 128, 3, 3),\n",
    " 'block3.0.conv_bn1.bn.weight',\t(256,),\t 'layer3.0.bn1.weight',\t(256,),\n",
    " 'block3.0.conv_bn1.bn.bias',\t(256,),\t 'layer3.0.bn1.bias',\t(256,),\n",
    " 'block3.0.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.0.bn1.running_mean',\t(256,),\n",
    " 'block3.0.conv_bn1.bn.running_var',\t(256,),\t 'layer3.0.bn1.running_var',\t(256,),\n",
    " 'block3.0.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.0.conv2.weight',\t(256, 256, 3, 3),\n",
    " 'block3.0.conv_bn2.bn.weight',\t(256,),\t 'layer3.0.bn2.weight',\t(256,),\n",
    " 'block3.0.conv_bn2.bn.bias',\t(256,),\t 'layer3.0.bn2.bias',\t(256,),\n",
    " 'block3.0.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.0.bn2.running_mean',\t(256,),\n",
    " 'block3.0.conv_bn2.bn.running_var',\t(256,),\t 'layer3.0.bn2.running_var',\t(256,),\n",
    " 'block3.0.shortcut.conv.weight',\t(256, 128, 1, 1),\t 'layer3.0.downsample.0.weight',\t(256, 128, 1, 1),\n",
    " 'block3.0.shortcut.bn.weight',\t(256,),\t 'layer3.0.downsample.1.weight',\t(256,),\n",
    " 'block3.0.shortcut.bn.bias',\t(256,),\t 'layer3.0.downsample.1.bias',\t(256,),\n",
    " 'block3.0.shortcut.bn.running_mean',\t(256,),\t 'layer3.0.downsample.1.running_mean',\t(256,),\n",
    " 'block3.0.shortcut.bn.running_var',\t(256,),\t 'layer3.0.downsample.1.running_var',\t(256,),\n",
    " 'block3.1.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.1.conv1.weight',\t(256, 256, 3, 3),\n",
    " 'block3.1.conv_bn1.bn.weight',\t(256,),\t 'layer3.1.bn1.weight',\t(256,),\n",
    " 'block3.1.conv_bn1.bn.bias',\t(256,),\t 'layer3.1.bn1.bias',\t(256,),\n",
    " 'block3.1.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.1.bn1.running_mean',\t(256,),\n",
    " 'block3.1.conv_bn1.bn.running_var',\t(256,),\t 'layer3.1.bn1.running_var',\t(256,),\n",
    " 'block3.1.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.1.conv2.weight',\t(256, 256, 3, 3),\n",
    " 'block3.1.conv_bn2.bn.weight',\t(256,),\t 'layer3.1.bn2.weight',\t(256,),\n",
    " 'block3.1.conv_bn2.bn.bias',\t(256,),\t 'layer3.1.bn2.bias',\t(256,),\n",
    " 'block3.1.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.1.bn2.running_mean',\t(256,),\n",
    " 'block3.1.conv_bn2.bn.running_var',\t(256,),\t 'layer3.1.bn2.running_var',\t(256,),\n",
    " 'block3.2.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.2.conv1.weight',\t(256, 256, 3, 3),\n",
    " 'block3.2.conv_bn1.bn.weight',\t(256,),\t 'layer3.2.bn1.weight',\t(256,),\n",
    " 'block3.2.conv_bn1.bn.bias',\t(256,),\t 'layer3.2.bn1.bias',\t(256,),\n",
    " 'block3.2.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.2.bn1.running_mean',\t(256,),\n",
    " 'block3.2.conv_bn1.bn.running_var',\t(256,),\t 'layer3.2.bn1.running_var',\t(256,),\n",
    " 'block3.2.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.2.conv2.weight',\t(256, 256, 3, 3),\n",
    " 'block3.2.conv_bn2.bn.weight',\t(256,),\t 'layer3.2.bn2.weight',\t(256,),\n",
    " 'block3.2.conv_bn2.bn.bias',\t(256,),\t 'layer3.2.bn2.bias',\t(256,),\n",
    " 'block3.2.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.2.bn2.running_mean',\t(256,),\n",
    " 'block3.2.conv_bn2.bn.running_var',\t(256,),\t 'layer3.2.bn2.running_var',\t(256,),\n",
    " 'block3.3.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.3.conv1.weight',\t(256, 256, 3, 3),\n",
    " 'block3.3.conv_bn1.bn.weight',\t(256,),\t 'layer3.3.bn1.weight',\t(256,),\n",
    " 'block3.3.conv_bn1.bn.bias',\t(256,),\t 'layer3.3.bn1.bias',\t(256,),\n",
    " 'block3.3.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.3.bn1.running_mean',\t(256,),\n",
    " 'block3.3.conv_bn1.bn.running_var',\t(256,),\t 'layer3.3.bn1.running_var',\t(256,),\n",
    " 'block3.3.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.3.conv2.weight',\t(256, 256, 3, 3),\n",
    " 'block3.3.conv_bn2.bn.weight',\t(256,),\t 'layer3.3.bn2.weight',\t(256,),\n",
    " 'block3.3.conv_bn2.bn.bias',\t(256,),\t 'layer3.3.bn2.bias',\t(256,),\n",
    " 'block3.3.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.3.bn2.running_mean',\t(256,),\n",
    " 'block3.3.conv_bn2.bn.running_var',\t(256,),\t 'layer3.3.bn2.running_var',\t(256,),\n",
    " 'block3.4.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.4.conv1.weight',\t(256, 256, 3, 3),\n",
    " 'block3.4.conv_bn1.bn.weight',\t(256,),\t 'layer3.4.bn1.weight',\t(256,),\n",
    " 'block3.4.conv_bn1.bn.bias',\t(256,),\t 'layer3.4.bn1.bias',\t(256,),\n",
    " 'block3.4.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.4.bn1.running_mean',\t(256,),\n",
    " 'block3.4.conv_bn1.bn.running_var',\t(256,),\t 'layer3.4.bn1.running_var',\t(256,),\n",
    " 'block3.4.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.4.conv2.weight',\t(256, 256, 3, 3),\n",
    " 'block3.4.conv_bn2.bn.weight',\t(256,),\t 'layer3.4.bn2.weight',\t(256,),\n",
    " 'block3.4.conv_bn2.bn.bias',\t(256,),\t 'layer3.4.bn2.bias',\t(256,),\n",
    " 'block3.4.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.4.bn2.running_mean',\t(256,),\n",
    " 'block3.4.conv_bn2.bn.running_var',\t(256,),\t 'layer3.4.bn2.running_var',\t(256,),\n",
    " 'block3.5.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.5.conv1.weight',\t(256, 256, 3, 3),\n",
    " 'block3.5.conv_bn1.bn.weight',\t(256,),\t 'layer3.5.bn1.weight',\t(256,),\n",
    " 'block3.5.conv_bn1.bn.bias',\t(256,),\t 'layer3.5.bn1.bias',\t(256,),\n",
    " 'block3.5.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.5.bn1.running_mean',\t(256,),\n",
    " 'block3.5.conv_bn1.bn.running_var',\t(256,),\t 'layer3.5.bn1.running_var',\t(256,),\n",
    " 'block3.5.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.5.conv2.weight',\t(256, 256, 3, 3),\n",
    " 'block3.5.conv_bn2.bn.weight',\t(256,),\t 'layer3.5.bn2.weight',\t(256,),\n",
    " 'block3.5.conv_bn2.bn.bias',\t(256,),\t 'layer3.5.bn2.bias',\t(256,),\n",
    " 'block3.5.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.5.bn2.running_mean',\t(256,),\n",
    " 'block3.5.conv_bn2.bn.running_var',\t(256,),\t 'layer3.5.bn2.running_var',\t(256,),\n",
    " 'block4.0.conv_bn1.conv.weight',\t(512, 256, 3, 3),\t 'layer4.0.conv1.weight',\t(512, 256, 3, 3),\n",
    " 'block4.0.conv_bn1.bn.weight',\t(512,),\t 'layer4.0.bn1.weight',\t(512,),\n",
    " 'block4.0.conv_bn1.bn.bias',\t(512,),\t 'layer4.0.bn1.bias',\t(512,),\n",
    " 'block4.0.conv_bn1.bn.running_mean',\t(512,),\t 'layer4.0.bn1.running_mean',\t(512,),\n",
    " 'block4.0.conv_bn1.bn.running_var',\t(512,),\t 'layer4.0.bn1.running_var',\t(512,),\n",
    " 'block4.0.conv_bn2.conv.weight',\t(512, 512, 3, 3),\t 'layer4.0.conv2.weight',\t(512, 512, 3, 3),\n",
    " 'block4.0.conv_bn2.bn.weight',\t(512,),\t 'layer4.0.bn2.weight',\t(512,),\n",
    " 'block4.0.conv_bn2.bn.bias',\t(512,),\t 'layer4.0.bn2.bias',\t(512,),\n",
    " 'block4.0.conv_bn2.bn.running_mean',\t(512,),\t 'layer4.0.bn2.running_mean',\t(512,),\n",
    " 'block4.0.conv_bn2.bn.running_var',\t(512,),\t 'layer4.0.bn2.running_var',\t(512,),\n",
    " 'block4.0.shortcut.conv.weight',\t(512, 256, 1, 1),\t 'layer4.0.downsample.0.weight',\t(512, 256, 1, 1),\n",
    " 'block4.0.shortcut.bn.weight',\t(512,),\t 'layer4.0.downsample.1.weight',\t(512,),\n",
    " 'block4.0.shortcut.bn.bias',\t(512,),\t 'layer4.0.downsample.1.bias',\t(512,),\n",
    " 'block4.0.shortcut.bn.running_mean',\t(512,),\t 'layer4.0.downsample.1.running_mean',\t(512,),\n",
    " 'block4.0.shortcut.bn.running_var',\t(512,),\t 'layer4.0.downsample.1.running_var',\t(512,),\n",
    " 'block4.1.conv_bn1.conv.weight',\t(512, 512, 3, 3),\t 'layer4.1.conv1.weight',\t(512, 512, 3, 3),\n",
    " 'block4.1.conv_bn1.bn.weight',\t(512,),\t 'layer4.1.bn1.weight',\t(512,),\n",
    " 'block4.1.conv_bn1.bn.bias',\t(512,),\t 'layer4.1.bn1.bias',\t(512,),\n",
    " 'block4.1.conv_bn1.bn.running_mean',\t(512,),\t 'layer4.1.bn1.running_mean',\t(512,),\n",
    " 'block4.1.conv_bn1.bn.running_var',\t(512,),\t 'layer4.1.bn1.running_var',\t(512,),\n",
    " 'block4.1.conv_bn2.conv.weight',\t(512, 512, 3, 3),\t 'layer4.1.conv2.weight',\t(512, 512, 3, 3),\n",
    " 'block4.1.conv_bn2.bn.weight',\t(512,),\t 'layer4.1.bn2.weight',\t(512,),\n",
    " 'block4.1.conv_bn2.bn.bias',\t(512,),\t 'layer4.1.bn2.bias',\t(512,),\n",
    " 'block4.1.conv_bn2.bn.running_mean',\t(512,),\t 'layer4.1.bn2.running_mean',\t(512,),\n",
    " 'block4.1.conv_bn2.bn.running_var',\t(512,),\t 'layer4.1.bn2.running_var',\t(512,),\n",
    " 'block4.2.conv_bn1.conv.weight',\t(512, 512, 3, 3),\t 'layer4.2.conv1.weight',\t(512, 512, 3, 3),\n",
    " 'block4.2.conv_bn1.bn.weight',\t(512,),\t 'layer4.2.bn1.weight',\t(512,),\n",
    " 'block4.2.conv_bn1.bn.bias',\t(512,),\t 'layer4.2.bn1.bias',\t(512,),\n",
    " 'block4.2.conv_bn1.bn.running_mean',\t(512,),\t 'layer4.2.bn1.running_mean',\t(512,),\n",
    " 'block4.2.conv_bn1.bn.running_var',\t(512,),\t 'layer4.2.bn1.running_var',\t(512,),\n",
    " 'block4.2.conv_bn2.conv.weight',\t(512, 512, 3, 3),\t 'layer4.2.conv2.weight',\t(512, 512, 3, 3),\n",
    " 'block4.2.conv_bn2.bn.weight',\t(512,),\t 'layer4.2.bn2.weight',\t(512,),\n",
    " 'block4.2.conv_bn2.bn.bias',\t(512,),\t 'layer4.2.bn2.bias',\t(512,),\n",
    " 'block4.2.conv_bn2.bn.running_mean',\t(512,),\t 'layer4.2.bn2.running_mean',\t(512,),\n",
    " 'block4.2.conv_bn2.bn.running_var',\t(512,),\t 'layer4.2.bn2.running_var',\t(512,),\n",
    " 'logit.weight',\t(1000, 512),\t 'fc.weight',\t(1000, 512),\n",
    " 'logit.bias',\t(1000,),\t 'fc.bias',\t(1000,),\n",
    "\n",
    "]\n",
    "\n",
    "###############################################################################\n",
    "class ConvBn2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=3, padding=1, stride=1):\n",
    "        super(ConvBn2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n",
    "        self.bn   = nn.BatchNorm2d(out_channel, eps=1e-5)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############  resnext50 pyramid feature net #######################################\n",
    "# https://github.com/Hsuxu/ResNeXt/blob/master/models.py\n",
    "# https://github.com/D-X-Y/ResNeXt-DenseNet/blob/master/models/resnext.py\n",
    "# https://github.com/miraclewkf/ResNeXt-PyTorch/blob/master/resnext.py\n",
    "\n",
    "\n",
    "# bottleneck type C\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channel, channel, out_channel, stride=1, is_shortcut=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.is_shortcut = is_shortcut\n",
    "\n",
    "        self.conv_bn1 = ConvBn2d(in_channel,    channel, kernel_size=3, padding=1, stride=stride)\n",
    "        self.conv_bn2 = ConvBn2d(   channel,out_channel, kernel_size=3, padding=1, stride=1)\n",
    "\n",
    "        if is_shortcut:\n",
    "            self.shortcut = ConvBn2d(in_channel, out_channel, kernel_size=1, padding=0, stride=stride)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = F.relu(self.conv_bn1(x),inplace=True)\n",
    "        z = self.conv_bn2(z)\n",
    "\n",
    "        if self.is_shortcut:\n",
    "            x = self.shortcut(x)\n",
    "\n",
    "        z += x\n",
    "        z = F.relu(z,inplace=True)\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResNet34(nn.Module):\n",
    "\n",
    "    def __init__(self, num_class=1000 ):\n",
    "        super(ResNet34, self).__init__()\n",
    "\n",
    "\n",
    "        self.block0  = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, padding=3, stride=2, bias=False),\n",
    "            BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.block1  = nn.Sequential(\n",
    "             nn.MaxPool2d(kernel_size=3, padding=1, stride=2),\n",
    "             BasicBlock( 64, 64, 64, stride=1, is_shortcut=False,),\n",
    "          * [BasicBlock( 64, 64, 64, stride=1, is_shortcut=False,) for i in range(1,3)],\n",
    "        )\n",
    "        self.block2  = nn.Sequential(\n",
    "             BasicBlock( 64,128,128, stride=2, is_shortcut=True, ),\n",
    "          * [BasicBlock(128,128,128, stride=1, is_shortcut=False,) for i in range(1,4)],\n",
    "        )\n",
    "        self.block3  = nn.Sequential(\n",
    "             BasicBlock(128,256,256, stride=2, is_shortcut=True, ),\n",
    "          * [BasicBlock(256,256,256, stride=1, is_shortcut=False,) for i in range(1,6)],\n",
    "        )\n",
    "        self.block4 = nn.Sequential(\n",
    "             BasicBlock(256,512,512, stride=2, is_shortcut=True, ),\n",
    "          * [BasicBlock(512,512,512, stride=1, is_shortcut=False,) for i in range(1,3)],\n",
    "        )\n",
    "        self.logit = nn.Linear(512,num_class)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = len(x)\n",
    "\n",
    "        x = self.block0(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n",
    "        logit = self.logit(x)\n",
    "        return logit\n",
    "class Resnet34_classification(nn.Module):\n",
    "    def __init__(self,num_class=4):\n",
    "        super(Resnet34_classification, self).__init__()\n",
    "        e = ResNet34()\n",
    "        self.block = nn.ModuleList([\n",
    "            e.block0,\n",
    "            e.block1,\n",
    "            e.block2,\n",
    "            e.block3,\n",
    "            e.block4,\n",
    "        ])\n",
    "        e = None  #dropped\n",
    "        self.feature = nn.Conv2d(512,32, kernel_size=1) #dummy conv for dim reduction\n",
    "        self.logit = nn.Conv2d(32,num_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size,C,H,W = x.shape\n",
    "\n",
    "        for i in range( len(self.block)):\n",
    "            x = self.block[i](x)\n",
    "            #print(i, x.shape)\n",
    "\n",
    "        x = F.dropout(x,0.5,training=self.training)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = self.feature(x)\n",
    "        logit = self.logit(x)\n",
    "        return logit\n",
    "    \n",
    "\n",
    "def getHengsClassificationModel():\n",
    "    model_classification = Resnet34_classification()\n",
    "    model_classification.load_state_dict(torch.load('output_weights/00007500_model.pth'))\n",
    "    return model_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBackbone(mx.gluon.HybridBlock):\n",
    "    def __init__(self, backbone='resnet50', pretrained_base=True,dilated=True, **kwargs):\n",
    "        super(ResNetBackbone, self).__init__()\n",
    "\n",
    "        with self.name_scope():\n",
    "            if backbone == 'resnet50':\n",
    "                pretrained = resnet50_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n",
    "            elif backbone == 'resnet101':\n",
    "                pretrained = resnet101_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n",
    "            elif backbone == 'resnet152':\n",
    "                pretrained = resnet152_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n",
    "            else:\n",
    "                raise RuntimeError(f'unknown backbone: {backbone}')\n",
    "\n",
    "            self.conv1 = pretrained.conv1\n",
    "            self.bn1 = pretrained.bn1\n",
    "            self.relu = pretrained.relu\n",
    "            self.maxpool = pretrained.maxpool\n",
    "            self.layer1 = pretrained.layer1\n",
    "            self.layer2 = pretrained.layer2\n",
    "            self.layer3 = pretrained.layer3\n",
    "            self.layer4 = pretrained.layer4\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        c1 = self.layer1(x)\n",
    "        c2 = self.layer2(c1)\n",
    "        c3 = self.layer3(c2)\n",
    "        c4 = self.layer4(c3)\n",
    "\n",
    "        return c1, c2, c3, c4\n",
    "\n",
    "class ResNetFPN(mx.gluon.HybridBlock):\n",
    "    def __init__(self, backbone= 'resnet50', backbone_lr_mult=0.1, **kwargs):\n",
    "        super(ResNetFPN, self).__init__()\n",
    "\n",
    "        self.backbone_name = backbone\n",
    "        self.backbone_lr_mult = backbone_lr_mult\n",
    "        self._kwargs = kwargs\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.backbone = ResNetBackbone(backbone=self.backbone_name, pretrained_base=False, dilated=False, **kwargs)\n",
    "\n",
    "            self.head = _FPNHead(output_channels=256, **kwargs)\n",
    "\n",
    "    def load_pretrained_weights(self):\n",
    "        pretrained = ResNetBackbone(backbone=self.backbone_name, pretrained_base=True, dilated=False, **self._kwargs)\n",
    "        backbone_params = self.backbone.collect_params()\n",
    "        pretrained_weights = pretrained.collect_params()\n",
    "        for k, v in pretrained_weights.items():\n",
    "            param_name = backbone_params.prefix + k[len(pretrained_weights.prefix):]\n",
    "            backbone_params[param_name].set_data(v.data())\n",
    "\n",
    "        self.backbone.collect_params().setattr('lr_mult', self.backbone_lr_mult)\n",
    "\n",
    "    def hybrid_forward(self,F, x):\n",
    "        c1, c2, c3, c4 = self.backbone(x)\n",
    "        p1, p2, p3, p4 = self.head(c1, c2, c3, c4)\n",
    "\n",
    "        return p1, p2, p3, p4\n",
    "\n",
    "class ResNetUnet(mx.gluon.HybridBlock):\n",
    "    def __init__(self, backbone= 'resnet50', backbone_lr_mult=0.1, cls_branch=False, **kwargs):\n",
    "        super(ResNetUnet, self).__init__()\n",
    "\n",
    "        self.backbone_name = backbone\n",
    "        self.backbone_lr_mult = backbone_lr_mult\n",
    "        self.cls_branch = cls_branch\n",
    "        self._kwargs = kwargs\n",
    "        \n",
    "        with self.name_scope():\n",
    "            self.backbone = ResNetBackbone(backbone=self.backbone_name, pretrained_base=False, dilated=False, **kwargs)\n",
    "\n",
    "            self.head = _UnetHead(**kwargs)\n",
    "\n",
    "    def load_pretrained_weights(self):\n",
    "        pretrained = ResNetBackbone(backbone=self.backbone_name, pretrained_base=True, dilated=False, **self._kwargs)\n",
    "        backbone_params = self.backbone.collect_params()\n",
    "        pretrained_weights = pretrained.collect_params()\n",
    "        for k, v in pretrained_weights.items():\n",
    "            param_name = backbone_params.prefix + k[len(pretrained_weights.prefix):]\n",
    "            backbone_params[param_name].set_data(v.data())\n",
    "\n",
    "        self.backbone.collect_params().setattr('lr_mult', self.backbone_lr_mult)\n",
    "\n",
    "    def hybrid_forward(self,F, x):\n",
    "        c1, c2, c3, c4 = self.backbone(x)\n",
    "        out = self.head(c1, c2, c3, c4)\n",
    "        if self.cls_branch:\n",
    "            logits = F.max(F.mean(out, axis=1), axis=(1, 2))\n",
    "            return out, logits\n",
    "        return out\n",
    "\n",
    "class _DecoderBlock(HybridBlock):\n",
    "    def __init__(self, output_channels, norm_layer=nn_mxnet.BatchNorm):\n",
    "        super(_DecoderBlock, self).__init__()\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.block = nn_mxnet.HybridSequential()\n",
    "            self.block.add(ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer))\n",
    "            self.block.add(ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer))\n",
    "\n",
    "    def hybrid_forward(self, F, x, y=None):\n",
    "\n",
    "        if y is not None:\n",
    "            x = F.contrib.BilinearResize2D(x, scale_height=2, scale_width=2)\n",
    "            x = F.concat(x, y, dim=1)\n",
    "        out = self.block(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class _UnetHead(HybridBlock):\n",
    "    def __init__(self, num_classes, output_channels=[256, 128, 64, 32], scale=4, norm_layer=nn_mxnet.BatchNorm):\n",
    "        super(_UnetHead, self).__init__()\n",
    "        \n",
    "        self.scale = scale\n",
    "        with self.name_scope():\n",
    "            self.block4 = _DecoderBlock(output_channels[0], norm_layer=norm_layer)\n",
    "            self.block3 = _DecoderBlock(output_channels[1], norm_layer=norm_layer)\n",
    "            self.block2 = _DecoderBlock(output_channels[2], norm_layer=norm_layer)\n",
    "            self.block1 = _DecoderBlock(output_channels[3], norm_layer=norm_layer)\n",
    "            self.postprocess_block = nn_mxnet.Conv2D(num_classes, kernel_size=1)\n",
    "\n",
    "    def hybrid_forward(self, F, c1, c2, c3, c4):\n",
    "\n",
    "        p4 = self.block4(c4)\n",
    "        p3 = self.block3(p4, c3)\n",
    "        p2 = self.block2(p3, c2)\n",
    "        p1 = self.block1(p2, c1)\n",
    "        if self.scale > 1:\n",
    "            p1 = F.contrib.BilinearResize2D(p1, scale_height=self.scale, scale_width=self.scale)\n",
    "        out = self.postprocess_block(p1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class _FPNHead(HybridBlock):\n",
    "    def __init__(self, output_channels=256, norm_layer=nn_mxnet.BatchNorm):\n",
    "        super(_FPNHead, self).__init__()\n",
    "        self._hdsize = {}\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.block4 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n",
    "            self.block3 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n",
    "            self.block2 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n",
    "            self.block1 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n",
    "\n",
    "    def hybrid_forward(self, F, c1, c2, c3, c4):\n",
    "        p4 = self.block4(c4)\n",
    "        p3 = self._resize_as(F, 'id_1', p4, c3) + self.block3(c3)\n",
    "        p2 = self._resize_as(F, 'id_2', p3, c2) + self.block2(c2)\n",
    "        p1 = self._resize_as(F, 'id_3', p2, c1) + self.block1(c1)\n",
    "\n",
    "        return p1, p2, p3, p4\n",
    "\n",
    "    def _resize_as(self, F, name, x, y):\n",
    "        h_key = name + '_h'\n",
    "        w_key = name + '_w'\n",
    "\n",
    "        if hasattr(y, 'shape'):\n",
    "            _, _, h, w = y.shape\n",
    "            _, _, h2, w2 = x.shape\n",
    "\n",
    "            if h == h2 and w == w2:\n",
    "                h = 0\n",
    "                w = 0\n",
    "\n",
    "            self._hdsize[h_key] = h\n",
    "            self._hdsize[w_key] = w\n",
    "        else:\n",
    "            h, w = self._hdsize[h_key], self._hdsize[w_key]\n",
    "\n",
    "        if h == 0 and w == 0:\n",
    "            return x\n",
    "        else:\n",
    "            return F.contrib.BilinearResize2D(x, height=h, width=w)\n",
    "\n",
    "\n",
    "class SemanticFPNHead(HybridBlock):\n",
    "    def __init__(self, num_classes, output_channels=128, norm_layer=nn_mxnet.BatchNorm):\n",
    "        super(SemanticFPNHead, self).__init__()\n",
    "        self._hdsize = {}\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.block4_1 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n",
    "            self.block4_2 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n",
    "            self.block4_3 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n",
    "\n",
    "            self.block3_1 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n",
    "            self.block3_2 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n",
    "\n",
    "            self.block2 = ConvBlock(output_channels, kernel_size=3, padding=1, norm_layer=norm_layer)\n",
    "            self.block1 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n",
    "\n",
    "            self.postprocess_block = nn_mxnet.Conv2D(num_classes, kernel_size=1)\n",
    "\n",
    "    def hybrid_forward(self, F, c1, c2, c3, c4):\n",
    "        out4 = self._resize_as(F, 'id_1', self.block4_1(c4), c3)\n",
    "        out4 = self._resize_as(F, 'id_2', self.block4_2(out4), c2)\n",
    "        out4 = self._resize_as(F, 'id_3', self.block4_3(out4), c1)\n",
    "\n",
    "        out3 = self._resize_as(F, 'id_4', self.block3_1(c3), c2)\n",
    "        out3 = self._resize_as(F, 'id_5', self.block3_2(out3), c1)\n",
    "\n",
    "        out2 = self._resize_as(F, 'id_6', self.block2(c2), c1)\n",
    "\n",
    "        out1 = self.block1(c1)\n",
    "\n",
    "        out = out1 + out2 + out3 + out4\n",
    "\n",
    "        out = self.postprocess_block(out)\n",
    "        out = F.contrib.BilinearResize2D(out,scale_height=4,scale_width=4)\n",
    "        return out\n",
    "\n",
    "    def _resize_as(self, F,name, x, y):\n",
    "        h_key = name + '_h'\n",
    "        w_key = name + '_w'\n",
    "\n",
    "        if hasattr(y, 'shape'):\n",
    "            _, _, h, w = y.shape\n",
    "            _, _, h2, w2 = x.shape\n",
    "\n",
    "            if h == h2 and w == w2:\n",
    "                h = 0\n",
    "                w = 0\n",
    "\n",
    "            self._hdsize[h_key]=h\n",
    "            self._hdsize[w_key]=w\n",
    "        else:\n",
    "            h, w = self._hdsize[h_key], self._hdsize[w_key]\n",
    "\n",
    "        if h == 0 and w == 0:\n",
    "            return x\n",
    "        else:\n",
    "            return F.contrib.BilinearResize2D(x,height=h,width=w)\n",
    "\n",
    "\n",
    "class ConvBlock(HybridBlock):\n",
    "    def __init__(self, output_channels, kernel_size, padding=0, activation='relu', norm_layer=nn_mxnet.BatchNorm):\n",
    "        super().__init__()\n",
    "        self.body = nn_mxnet.HybridSequential()\n",
    "        self.body.add(\n",
    "            nn_mxnet.Conv2D(output_channels, kernel_size=kernel_size, padding=padding, activation=activation),\n",
    "            norm_layer(in_channels=output_channels)\n",
    "        )\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        return self.body(x)\n",
    "\n",
    "class SteelUnet(HybridBlock):\n",
    "    \n",
    "    def __init__(self, n_classes=5, ctx=mx.cpu()):\n",
    "        super().__init__()\n",
    "        with self.name_scope():\n",
    "            self.feature_extractor = ResNetFPN()\n",
    "            self.segment_head = SemanticFPNHead(num_classes=n_classes)\n",
    "    def hybrid_forward(self, F, x):\n",
    "        fpn_feature = self.feature_extractor(x)\n",
    "        segment_out = self.segment_head(*fpn_feature)\n",
    "        return segment_out\n",
    "    \n",
    "def getmxnetunet():\n",
    "    ctx = mx.gpu()\n",
    "    unet = ResNetUnet(output_channels=[256, 128, 64, 32], num_classes=5)\n",
    "    unet.load_parameters('out/unet_34_-1.params')\n",
    "    unet.collect_params().reset_ctx(ctx)\n",
    "    return unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "MXNetError",
     "evalue": "[21:19:01] src/io/local_filesys.cc:209: Check failed: allow_null:  LocalFileSystem::Open \"../input/mxnet-gluon-baseline/unet_34_-1.params\": No such file or directory\nStack trace:\n  [bt] (0) /home/avinash/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x2ea6de8) [0x7fdf8cb17de8]\n  [bt] (1) /home/avinash/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x2e9e9ba) [0x7fdf8cb0f9ba]\n  [bt] (2) /home/avinash/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(MXNDArrayLoad+0x23d) [0x7fdf8c19e5ed]\n  [bt] (3) /home/avinash/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7fe0474eeec0]\n  [bt] (4) /home/avinash/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x7fe0474ee87d]\n  [bt] (5) /home/avinash/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fe047703ede]\n  [bt] (6) /home/avinash/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12914) [0x7fe047704914]\n  [bt] (7) /home/avinash/anaconda3/bin/python(_PyObject_FastCallKeywords+0x49b) [0x55ba16eb68fb]\n  [bt] (8) /home/avinash/anaconda3/bin/python(_PyEval_EvalFrameDefault+0x52f8) [0x55ba16f1a6e8]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-df417c849a64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetmxnetunet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-f414c16238f8>\u001b[0m in \u001b[0;36mgetmxnetunet\u001b[0;34m()\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0munet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNetUnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0munet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/mxnet-gluon-baseline/unet_34_-1.params'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0munet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mxnet/gluon/block.py\u001b[0m in \u001b[0;36mload_parameters\u001b[0;34m(self, filename, ctx, allow_missing, ignore_extra, cast_dtype, dtype_source)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;34m<\u001b[0m\u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mmxnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincubator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mgluon\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msave_load_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \"\"\"\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collect_params_with_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mxnet/ndarray/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m    173\u001b[0m                                   \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                                   \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_name_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                                   ctypes.byref(names)))\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout_name_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_ndarray_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNDArrayHandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/mxnet/base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \"\"\"\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMXNetError\u001b[0m: [21:19:01] src/io/local_filesys.cc:209: Check failed: allow_null:  LocalFileSystem::Open \"../input/mxnet-gluon-baseline/unet_34_-1.params\": No such file or directory\nStack trace:\n  [bt] (0) /home/avinash/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x2ea6de8) [0x7fdf8cb17de8]\n  [bt] (1) /home/avinash/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(+0x2e9e9ba) [0x7fdf8cb0f9ba]\n  [bt] (2) /home/avinash/anaconda3/lib/python3.7/site-packages/mxnet/libmxnet.so(MXNDArrayLoad+0x23d) [0x7fdf8c19e5ed]\n  [bt] (3) /home/avinash/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7fe0474eeec0]\n  [bt] (4) /home/avinash/anaconda3/lib/python3.7/lib-dynload/../../libffi.so.6(ffi_call+0x22d) [0x7fe0474ee87d]\n  [bt] (5) /home/avinash/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2ce) [0x7fe047703ede]\n  [bt] (6) /home/avinash/anaconda3/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0x12914) [0x7fe047704914]\n  [bt] (7) /home/avinash/anaconda3/bin/python(_PyObject_FastCallKeywords+0x49b) [0x55ba16eb68fb]\n  [bt] (8) /home/avinash/anaconda3/bin/python(_PyEval_EvalFrameDefault+0x52f8) [0x55ba16f1a6e8]\n\n"
     ]
    }
   ],
   "source": [
    "getmxnetunet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
