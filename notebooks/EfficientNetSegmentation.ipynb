{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob as glob\n",
    "import tqdm\n",
    "import shutil\n",
    "import math\n",
    "import requests\n",
    "import random\n",
    "import functools\n",
    "import logging\n",
    "import json\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from multiprocessing import Process\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from torch.utils.data import *\n",
    "from torch.utils.data.sampler import *\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel.data_parallel import data_parallel\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import albumentations as albu\n",
    "import albumentations.pytorch as AT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_label      = [ 0.75, 0.75, 0.5, 0.5]\n",
    "threshold_mask_pixel = [ 0.5, 0.5, 0.5, 0.5]\n",
    "threshold_mask_size  = [1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR    = '/home/avinash/Desktop/DL/severstal-steel-defect-detection/data'\n",
    "SUBMISSION_CSV_FILE = 'submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_PYTORCH_PAD = True\n",
    "IS_GATHER_EXCITE = False\n",
    "def drop_connect(x, probability, training):\n",
    "    if not training: return x\n",
    "\n",
    "    batch_size = len(x)\n",
    "    keep_probability = 1 - probability\n",
    "    noise = keep_probability\n",
    "    noise += torch.rand([batch_size, 1, 1, 1], dtype=x.dtype, device=x.device)\n",
    "    mask = torch.floor(noise)\n",
    "    x = x / keep_probability * mask\n",
    "\n",
    "    return x\n",
    "def upsize_add(x, lateral):\n",
    "    return F.interpolate(x, size=lateral.shape[2:], mode='nearest') + lateral\n",
    "\n",
    "def upsize(x, scale_factor=2):\n",
    "    x = F.interpolate(x, scale_factor=scale_factor, mode='nearest')\n",
    "    return x\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(len(x),-1)\n",
    "\n",
    "class Conv2dBn(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size=1, stride=1, zero_pad=[0,0,0,0], group=1):\n",
    "        super(Conv2dBn, self).__init__()\n",
    "        if IS_PYTORCH_PAD: zero_pad = [kernel_size//2]*4\n",
    "        self.pad  = nn.ZeroPad2d(zero_pad)\n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, padding=0, stride=stride, groups=group, bias=False)\n",
    "        self.bn   = nn.BatchNorm2d(out_channel, eps=1e-03, momentum=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pad (x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn  (x)\n",
    "        return x\n",
    "\n",
    "class SqueezeExcite(nn.Module):\n",
    "    def __init__(self, in_channel, reduction_channel, excite_size):\n",
    "        super(SqueezeExcite, self).__init__()\n",
    "        self.excite_size=excite_size\n",
    "\n",
    "        self.squeeze = nn.Conv2d(in_channel, reduction_channel, kernel_size=1, padding=0)\n",
    "        self.excite  = nn.Conv2d(reduction_channel, in_channel, kernel_size=1, padding=0)\n",
    "        self.act = Swish()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if IS_GATHER_EXCITE:\n",
    "            s = F.avg_pool2d(x, kernel_size=self.excite_size)\n",
    "        else:\n",
    "            s = F.adaptive_avg_pool2d(x,1)\n",
    "\n",
    "        s = self.act(self.squeeze(s))\n",
    "        s = torch.sigmoid(self.excite(s))\n",
    "\n",
    "        if IS_GATHER_EXCITE:\n",
    "            s = F.interpolate(s, size=(x.shape[2],x.shape[3]), mode='nearest')\n",
    "\n",
    "        x = s*x\n",
    "        return x\n",
    "\n",
    "class EfficientBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel, channel, out_channel, kernel_size, stride, zero_pad, excite_size, drop_connect_rate):\n",
    "        super().__init__()\n",
    "        self.is_shortcut = stride == 1 and in_channel == out_channel\n",
    "        self.drop_connect_rate = drop_connect_rate\n",
    "\n",
    "        if in_channel == channel:\n",
    "            self.bottleneck = nn.Sequential(\n",
    "                Conv2dBn(   channel, channel, kernel_size=kernel_size, stride=stride, zero_pad=zero_pad, group=channel),\n",
    "                Swish(),\n",
    "                SqueezeExcite(channel, in_channel//4, excite_size) if excite_size>0\n",
    "                else Identity(),\n",
    "                Conv2dBn(channel, out_channel, kernel_size=1, stride=1),\n",
    "            )\n",
    "        else:\n",
    "            self.bottleneck = nn.Sequential(\n",
    "                Conv2dBn(in_channel, channel, kernel_size=1, stride=1),\n",
    "                Swish(),\n",
    "                Conv2dBn(   channel, channel, kernel_size=kernel_size, stride=stride, zero_pad=zero_pad, group=channel),\n",
    "                Swish(),\n",
    "                SqueezeExcite(channel, in_channel//4, excite_size) if excite_size>0\n",
    "                else Identity(),\n",
    "                Conv2dBn(channel, out_channel, kernel_size=1, stride=1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = self.bottleneck(x)\n",
    "\n",
    "        if self.is_shortcut:\n",
    "            if self.training: b = drop_connect(b, self.drop_connect_rate, True)\n",
    "            x = b + x\n",
    "        else:\n",
    "            x = b\n",
    "        return x\n",
    "\n",
    "class ConvGnUp2d(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, num_group=32, kernel_size=3, padding=1, stride=1):\n",
    "        super(ConvGnUp2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n",
    "        self.gn   = nn.GroupNorm(num_group,out_channel)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = self.gn(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class EfficientNetB5(nn.Module):\n",
    "\n",
    "    def __init__(self, drop_connect_rate=0.4):\n",
    "        super(EfficientNetB5, self).__init__()\n",
    "        d = drop_connect_rate\n",
    "\n",
    "        # bottom-top\n",
    "        self.stem  = nn.Sequential(\n",
    "            Conv2dBn(3,48, kernel_size=3,stride=2,zero_pad=[0,1,0,1]),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "               EfficientBlock( 48,  48,  24, kernel_size=3, stride=1, zero_pad=[1,1,1,1], excite_size=128, drop_connect_rate=d*1/7),\n",
    "            * [EfficientBlock( 24,  24,  24, kernel_size=3, stride=1, zero_pad=[1,1,1,1], excite_size=128, drop_connect_rate=d*1/7) for i in range(1,3)],\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "               EfficientBlock( 24, 144,  40, kernel_size=3, stride=2, zero_pad=[0,1,0,1], excite_size= 64, drop_connect_rate=d*2/7),\n",
    "            * [EfficientBlock( 40, 240,  40, kernel_size=3, stride=1, zero_pad=[1,1,1,1], excite_size= 64, drop_connect_rate=d*2/7) for i in range(1,5)],\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "               EfficientBlock( 40, 240,  64, kernel_size=5, stride=2, zero_pad=[1,2,1,2], excite_size= 32, drop_connect_rate=d*3/7),\n",
    "            * [EfficientBlock( 64, 384,  64, kernel_size=5, stride=1, zero_pad=[2,2,2,2], excite_size= 32, drop_connect_rate=d*3/7) for i in range(1,5)],\n",
    "        )\n",
    "        self.block4 = nn.Sequential(\n",
    "               EfficientBlock( 64, 384, 128, kernel_size=3, stride=2, zero_pad=[0,1,0,1], excite_size= 16, drop_connect_rate=d*4/7),\n",
    "            * [EfficientBlock(128, 768, 128, kernel_size=3, stride=1, zero_pad=[1,1,1,1], excite_size= 16, drop_connect_rate=d*4/7) for i in range(1,7)],\n",
    "        )\n",
    "        self.block5 = nn.Sequential(\n",
    "               EfficientBlock(128, 768, 176, kernel_size=5, stride=1, zero_pad=[2,2,2,2], excite_size= 16, drop_connect_rate=d*5/7),\n",
    "            * [EfficientBlock(176,1056, 176, kernel_size=5, stride=1, zero_pad=[2,2,2,2], excite_size= 16, drop_connect_rate=d*5/7) for i in range(1,7)],\n",
    "        )\n",
    "        self.block6 = nn.Sequential(\n",
    "               EfficientBlock(176,1056, 304, kernel_size=5, stride=2, zero_pad=[1,2,1,2], excite_size=  8, drop_connect_rate=d*6/7),\n",
    "            * [EfficientBlock(304,1824, 304, kernel_size=5, stride=1, zero_pad=[2,2,2,2], excite_size=  8, drop_connect_rate=d*6/7) for i in range(1,9)],\n",
    "        )\n",
    "        self.block7 = nn.Sequential(\n",
    "               EfficientBlock(304,1824, 512, kernel_size=3, stride=1, zero_pad=[1,1,1,1], excite_size=  8, drop_connect_rate=d*7/7),\n",
    "            * [EfficientBlock(512,3072, 512, kernel_size=3, stride=1, zero_pad=[1,1,1,1], excite_size=  8, drop_connect_rate=d*7/7) for i in range(1,3)],\n",
    "        )\n",
    "\n",
    "        self.last = nn.Sequential(\n",
    "            Conv2dBn(512, 2048,kernel_size=1,stride=1),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        self.logit = nn.Linear(2048,1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = len(x)\n",
    "\n",
    "        x = self.stem(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "        x = self.last(x)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n",
    "        logit = self.logit(x)\n",
    "\n",
    "        return logit\n",
    "\n",
    "class EfficientNetB5Segmentation(nn.Module):\n",
    "\n",
    "    def __init__(self, num_class=4, drop_connect_rate=0.2):\n",
    "        super(EfficientNetB5Segmentation, self).__init__()\n",
    "\n",
    "        e = EfficientNetB5(drop_connect_rate)\n",
    "        self.stem   = e.stem\n",
    "        self.block1 = e.block1\n",
    "        self.block2 = e.block2\n",
    "        self.block3 = e.block3\n",
    "        self.block4 = e.block4\n",
    "        self.block5 = e.block5\n",
    "        self.block6 = e.block6\n",
    "        self.block7 = e.block7\n",
    "        self.last   = e.last\n",
    "        e = None  #dropped\n",
    "\n",
    "        #---\n",
    "        self.lateral0 = nn.Conv2d(2048, 64,  kernel_size=1, padding=0, stride=1)\n",
    "        self.lateral1 = nn.Conv2d( 176, 64,  kernel_size=1, padding=0, stride=1)\n",
    "        self.lateral2 = nn.Conv2d(  64, 64,  kernel_size=1, padding=0, stride=1)\n",
    "        self.lateral3 = nn.Conv2d(  40, 64,  kernel_size=1, padding=0, stride=1)\n",
    "\n",
    "        self.top1 = nn.Sequential(\n",
    "            ConvGnUp2d( 64, 64),\n",
    "            ConvGnUp2d( 64, 64),\n",
    "            ConvGnUp2d( 64, 64),\n",
    "        )\n",
    "        self.top2 = nn.Sequential(\n",
    "            ConvGnUp2d( 64, 64),\n",
    "            ConvGnUp2d( 64, 64),\n",
    "        )\n",
    "        self.top3 = nn.Sequential(\n",
    "            ConvGnUp2d( 64, 64),\n",
    "        )\n",
    "        self.top4 = nn.Sequential(\n",
    "            nn.Conv2d(64*3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.logit_mask = nn.Conv2d(64,1+num_class,kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size,C,H,W = x.shape\n",
    "\n",
    "        x = self.stem(x)            #; print('stem  ',x.shape)\n",
    "        x = self.block1(x)    ;x0=x #; print('block1',x.shape)\n",
    "        x = self.block2(x)    ;x1=x #; print('block2',x.shape)\n",
    "        x = self.block3(x)    ;x2=x #; print('block3',x.shape)\n",
    "        x = self.block4(x)          #; print('block4',x.shape)\n",
    "        x = self.block5(x)    ;x3=x #; print('block5',x.shape)\n",
    "        x = self.block6(x)          #; print('block6',x.shape)\n",
    "        x = self.block7(x)          #; print('block7',x.shape)\n",
    "        x = self.last(x)      ;x4=x #; print('last  ',x.shape)\n",
    "\n",
    "        # segment\n",
    "        t0 = self.lateral0(x4)\n",
    "        t1 = upsize_add(t0, self.lateral1(x3)) #16x16\n",
    "        t2 = upsize_add(t1, self.lateral2(x2)) #32x32\n",
    "        t3 = upsize_add(t2, self.lateral3(x1)) #64x64\n",
    "\n",
    "        t1 = self.top1(t1) #128x128\n",
    "        t2 = self.top2(t2) #128x128\n",
    "        t3 = self.top3(t3) #128x128\n",
    "\n",
    "        t = torch.cat([t1,t2,t3],1)\n",
    "        t = self.top4(t)\n",
    "        logit_mask = self.logit_mask(t)\n",
    "        logit_mask = F.interpolate(logit_mask, scale_factor=2.0, mode='bilinear', align_corners=False)\n",
    "\n",
    "        return logit_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_mask_to_probability_label(probability):\n",
    "    batch_size,num_class,H,W = probability.shape\n",
    "    probability = probability.permute(0, 2, 3, 1).contiguous().view(batch_size,-1, 5)\n",
    "    value, index = probability.max(1)\n",
    "    probability = value\n",
    "    return probability\n",
    "\n",
    "def image_to_input(image):\n",
    "    input = image.astype(np.float32)\n",
    "    input = input[...,::-1]/255\n",
    "    input = input.transpose(0,3,1,2)\n",
    "    # input[:,0] = (input[:,0]-IMAGE_RGB_MEAN[0])/IMAGE_RGB_STD[0]\n",
    "    # input[:,1] = (input[:,1]-IMAGE_RGB_MEAN[1])/IMAGE_RGB_STD[1]\n",
    "    # input[:,2] = (input[:,2]-IMAGE_RGB_MEAN[2])/IMAGE_RGB_STD[2]\n",
    "    return input\n",
    "\n",
    "\n",
    "class KaggleTestDataset(Dataset):\n",
    "    def __init__(self):\n",
    "\n",
    "        df =  pd.read_csv(DATA_DIR + '/sample_submission.csv')\n",
    "        df['ImageId'] = df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n",
    "        self.uid = df['ImageId'].unique().tolist()\n",
    "\n",
    "    def __str__(self):\n",
    "        string  = ''\n",
    "        string += '\\tlen = %d\\n'%len(self)\n",
    "        return string\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.uid)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(index)\n",
    "        image_id = self.uid[index]\n",
    "        image = cv2.imread(DATA_DIR + '/test_images/%s'%(image_id), cv2.IMREAD_COLOR)\n",
    "        return image, image_id\n",
    "\n",
    "\n",
    "def null_collate(batch):\n",
    "    batch_size = len(batch)\n",
    "    input = []\n",
    "    image_id = []\n",
    "    for b in range(batch_size):\n",
    "        input.append(batch[b][0])\n",
    "        image_id.append(batch[b][1])\n",
    "    input = np.stack(input)\n",
    "    input = torch.from_numpy(image_to_input(input))\n",
    "    return input, image_id\n",
    "\n",
    "\n",
    "\n",
    "### kaggle ##############################################################\n",
    "\n",
    "def post_process(mask, min_size):\n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "\n",
    "    predict = np.zeros((256, 1600), np.float32)\n",
    "    for c in range(1, num_component):\n",
    "        p = (component == c)\n",
    "        if p.sum() > min_size:\n",
    "            predict[p] = 1\n",
    "\n",
    "    return predict\n",
    "\n",
    "\n",
    "def run_length_encode(mask):\n",
    "    #https://www.kaggle.com/bigkizd/se-resnext50-89\n",
    "    m = mask.T.flatten()\n",
    "    if m.sum()==0:\n",
    "        rle=''\n",
    "    else:\n",
    "        m   = np.concatenate([[0], m, [0]])\n",
    "        run = np.where(m[1:] != m[:-1])[0] + 1\n",
    "        run[1::2] -= run[::2]\n",
    "        rle = ' '.join(str(r) for r in run)\n",
    "    return rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "\n",
    "def run_check_setup():\n",
    "    \n",
    "    ## load net\n",
    "    net = EfficientNetB5Segmentation().cuda()\n",
    "    net.load_state_dict(torch.load(CHECKPOINT_FILE, map_location=lambda storage, loc: storage),strict=True)\n",
    "\n",
    "    ## load data\n",
    "    image_id = ['004f40c73.jpg', '006f39c41.jpg', '00b7fb703.jpg', '00bbcd9af.jpg']\n",
    "    image=[]\n",
    "    for i in image_id:\n",
    "        m = cv2.imread(DATA_DIR +'/test_images/%s'%i)\n",
    "        image.append(m)\n",
    "    image = np.stack(image)\n",
    "    input = image_to_input(image)\n",
    "    input = torch.from_numpy(input).cuda()\n",
    "\n",
    "    #run here!\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        logit = net(input)\n",
    "        probability= torch.sigmoid(logit)\n",
    "\n",
    "    print('input: ',input.shape)\n",
    "    print('logit: ',logit.shape)\n",
    "    print('')\n",
    "    #---\n",
    "    input = input.data.cpu().numpy()\n",
    "    logit = logit.data.cpu().numpy()\n",
    "\n",
    "    if 1:\n",
    "        print(logit[0,0,:5,:5],'\\n')\n",
    "        print(logit[3,0,-5:,-5:],'\\n')\n",
    "        print(logit.mean(),logit.std(),logit.max(),logit.min(),'\\n')\n",
    "        print('')\n",
    "        print('---------------------')\n",
    "        print('')\n",
    "\n",
    "\n",
    "'''\n",
    "        \n",
    "input:  torch.Size([4, 3, 256, 1600])\n",
    "logit:  torch.Size([4, 5, 256, 1600])\n",
    "\n",
    "[[5.097667  5.279234  5.642368  5.863943  5.9439597]\n",
    " [5.122505  5.325696  5.7320786 5.976972  6.060377 ]\n",
    " [5.172181  5.4186206 5.9115    6.2030306 6.293213 ]\n",
    " [5.2060194 5.475979  6.0158978 6.331575  6.42301  ]\n",
    " [5.2240195 5.4977703 6.0452724 6.362605  6.449769 ]] \n",
    "\n",
    "[[5.0015    4.846141  4.578332  4.198073  4.0079427]\n",
    " [4.9045486 4.746123  4.4800625 4.1063666 3.9195185]\n",
    " [4.7522364 4.5993853 4.368871  4.0606937 3.9066048]\n",
    " [4.544562  4.4059286 4.244759  4.061054  3.9692013]\n",
    " [4.440725  4.3092003 4.182703  4.0612345 4.0004997]] \n",
    "\n",
    "0.035303652 3.9407735 10.5579815 -6.170837 \n",
    "\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_str(t, mode='min'):\n",
    "    if mode=='min':\n",
    "        t  = int(t)/60\n",
    "        hr = t//60\n",
    "        min = t%60\n",
    "        return '%2d hr %02d min'%(hr,min)\n",
    "\n",
    "    elif mode=='sec':\n",
    "        t   = int(t)\n",
    "        min = t//60\n",
    "        sec = t%60\n",
    "        return '%2d min %02d sec'%(min,sec)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_submission_csv(df):\n",
    "\n",
    "\n",
    "    text = ''\n",
    "    df['Class'] = df['ImageId_ClassId'].str[-1].astype(np.int32)\n",
    "    df['Label'] = (df['EncodedPixels']!='').astype(np.int32)\n",
    "    num_image = len(df)//4\n",
    "    num = len(df)\n",
    "\n",
    "    pos = (df['Label']==1).sum()\n",
    "    neg = num-pos\n",
    "\n",
    "\n",
    "    pos1 = ((df['Class']==1) & (df['Label']==1)).sum()\n",
    "    pos2 = ((df['Class']==2) & (df['Label']==1)).sum()\n",
    "    pos3 = ((df['Class']==3) & (df['Label']==1)).sum()\n",
    "    pos4 = ((df['Class']==4) & (df['Label']==1)).sum()\n",
    "\n",
    "    neg1 = num_image-pos1\n",
    "    neg2 = num_image-pos2\n",
    "    neg3 = num_image-pos3\n",
    "    neg4 = num_image-pos4\n",
    "\n",
    "\n",
    "    text += 'compare with LB probing ... \\n'\n",
    "    text += '\\t\\tnum_image = %5d(1801) \\n'%num_image\n",
    "    text += '\\t\\tnum  = %5d(7204) \\n'%num\n",
    "    text += '\\n'\n",
    "\n",
    "    text += '\\t\\tpos1 = %5d( 128)  %0.3f\\n'%(pos1,pos1/128)\n",
    "    text += '\\t\\tpos2 = %5d(  43)  %0.3f\\n'%(pos2,pos2/43)\n",
    "    text += '\\t\\tpos3 = %5d( 741)  %0.3f\\n'%(pos3,pos3/741)\n",
    "    text += '\\t\\tpos4 = %5d( 120)  %0.3f\\n'%(pos4,pos4/120)\n",
    "    text += '\\n'\n",
    "\n",
    "    text += '\\t\\tneg1 = %5d(1673)  %0.3f  %3d\\n'%(neg1,neg1/1673, neg1-1673)\n",
    "    text += '\\t\\tneg2 = %5d(1758)  %0.3f  %3d\\n'%(neg2,neg2/1758, neg2-1758)\n",
    "    text += '\\t\\tneg3 = %5d(1060)  %0.3f  %3d\\n'%(neg3,neg3/1060, neg3-1060)\n",
    "    text += '\\t\\tneg4 = %5d(1681)  %0.3f  %3d\\n'%(neg4,neg4/1681, neg4-1681)\n",
    "    text += '--------------------------------------------------\\n'\n",
    "    text += '\\t\\tneg  = %5d(6172)  %0.3f  %3d \\n'%(neg,neg/6172, neg-6172)\n",
    "    text += '\\n'\n",
    "\n",
    "    if 1:\n",
    "        #compare with reference\n",
    "        pass\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "\n",
    "def run_make_submission_csv():\n",
    "\n",
    "#     threshold_label      = [ 0.50, 0.50, 0.50, 0.50,]\n",
    "#     threshold_mask_pixel = [ 0.50, 0.50, 0.50, 0.50,]\n",
    "#     threshold_mask_size  = [ 1,  1,  1,  1,]\n",
    "    threshold_label = [0.75, 0.75, 0.5, 0.5]\n",
    "    threshold_mask_pixel = [0.5, 0.5, 0.5, 0.5]\n",
    "    threshold_mask_size  = [900, 900, 900, 2000]\n",
    "    ## load net\n",
    "    print('load net ...')\n",
    "#     lt = [40000, 36000, 45000, 34000, 42000, 41000]\n",
    "    \n",
    "    for i,file in enumerate(glob.glob('/home/avinash/efficientb5-fpn-crop256x400-foldb0c/checkpoint/*_model.pth')):\n",
    "        \n",
    "        val = file.split('/')[-1]\n",
    "        val = val.split('_')[0]\n",
    "        print(val)\n",
    "#         if int(val) in lt:\n",
    "        SUBMISSION_CSV_FILE = '/home/avinash/efficientb5-fpn-crop256x400-foldb0c/submit/submission'+val+'.csv'\n",
    "        net = EfficientNetB5Segmentation().cuda()\n",
    "        net.load_state_dict(torch.load(file, map_location=lambda storage, loc: storage))\n",
    "        print('')\n",
    "\n",
    "\n",
    "        ## load data\n",
    "        print('load data ...')\n",
    "        dataset = KaggleTestDataset()\n",
    "        print(dataset)\n",
    "        #exit(0)\n",
    "\n",
    "        loader  = DataLoader(\n",
    "            dataset,\n",
    "            sampler     = SequentialSampler(dataset),\n",
    "            batch_size  = 8,\n",
    "            drop_last   = False,\n",
    "            num_workers = 0,\n",
    "            pin_memory  = True,\n",
    "            collate_fn  = null_collate\n",
    "        )\n",
    "\n",
    "\n",
    "        ## start here ----------------------------------\n",
    "        image_id_class_id = []\n",
    "        encoded_pixel     = []\n",
    "\n",
    "\n",
    "        net.eval()\n",
    "\n",
    "        start_timer = timer()\n",
    "        for t,(input, image_id) in enumerate(loader):\n",
    "            if t%50==0:\n",
    "                print('\\r loader: t = %4d / %4d  %s  %s : %s'%(\n",
    "                      t, len(loader)-1, str(input.shape), image_id[0], time_to_str((timer() - start_timer),'sec'),\n",
    "                ),end='', flush=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                input = input.cuda()\n",
    "\n",
    "                logit = data_parallel(net,input)  #net(input)\n",
    "                probability = torch.softmax(logit,1)\n",
    "\n",
    "                probability_mask  = probability[:,1:]#just drop background\n",
    "                probability_label = probability_mask_to_probability_label(probability)[:,1:]\n",
    "\n",
    "\n",
    "            probability_mask  = probability_mask.data.cpu().numpy()\n",
    "            probability_label = probability_label.data.cpu().numpy()\n",
    "\n",
    "            batch_size = len(image_id)\n",
    "            for b in range(batch_size):\n",
    "                for c in range(4):\n",
    "                    rle=''\n",
    "\n",
    "                    predict_label = probability_label[b,c]>threshold_label[c]\n",
    "                    if predict_label:\n",
    "                        try:\n",
    "                            predict_mask = probability_mask[b,c] > threshold_mask_pixel[c]\n",
    "                            predict_mask = post_process(predict_mask, threshold_mask_size[c])\n",
    "                            rle = run_length_encode(predict_mask)\n",
    "\n",
    "                        except:\n",
    "                            print('An exception occurred : %s'%(image_id[b]+'_%d'%(c+1)))\n",
    "\n",
    "\n",
    "                    image_id_class_id.append(image_id[b]+'_%d'%(c+1))\n",
    "                    encoded_pixel.append(rle)\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(zip(image_id_class_id, encoded_pixel), columns=['ImageId_ClassId', 'EncodedPixels'])\n",
    "        df.to_csv(SUBMISSION_CSV_FILE, index=False)\n",
    "        print('')\n",
    "\n",
    "        ## print statistics ----\n",
    "        if 1:\n",
    "            text = summarise_submission_csv(df)\n",
    "            print(text)\n",
    "\n",
    "\n",
    "\n",
    "# main #################################################################\n",
    "if __name__ == '__main__':\n",
    "    #run_check_setup()\n",
    "    run_make_submission_csv()\n",
    "\n",
    "    print('\\nsucess!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset setup\n",
    "# class TestDataset(Dataset):\n",
    "#     '''Dataset for test prediction'''\n",
    "#     def __init__(self, root, df, mean, std):\n",
    "#         self.root = root\n",
    "#         df['ImageId'] = df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n",
    "#         self.fnames = df['ImageId'].unique().tolist()\n",
    "#         self.num_samples = len(self.fnames)\n",
    "#         self.transform = albu.Compose(\n",
    "#             [\n",
    "#                 albu.Normalize(),\n",
    "#                 AT.ToTensor()\n",
    "#             ]\n",
    "#         )\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         fname = self.fnames[idx]\n",
    "#         path = os.path.join(self.root, fname)\n",
    "#         image = cv2.imread(path)\n",
    "#         images = self.transform(image=image)[\"image\"]\n",
    "#         return fname, images\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.num_samples\n",
    "    \n",
    "# sample_submission_path = '../input/severstal-steel-defect-detection/sample_submission.csv'\n",
    "# test_data_folder = \"../input/severstal-steel-defect-detection/test_images\"\n",
    "\n",
    "# # hyperparameters\n",
    "# batch_size = 10\n",
    "\n",
    "# # mean and std\n",
    "# mean = (0.485, 0.456, 0.406)\n",
    "# std = (0.229, 0.224, 0.225)\n",
    "\n",
    "# df = pd.read_csv(sample_submission_path)\n",
    "\n",
    "# # dataloader\n",
    "# testset = DataLoader(\n",
    "#     TestDataset(test_data_folder, df, mean, std),\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False,\n",
    "#     num_workers=0,\n",
    "#     pin_memory=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# model = EfficientNetB5Segmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# path = params['model_name']\n",
    "# x = torch.load(path)\n",
    "# model.load_state_dict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# model = model.cuda()\n",
    "# def sharpen(p,t=0.5):\n",
    "#     if t!=0:\n",
    "#         return p**t\n",
    "#     else:\n",
    "#         return p\n",
    "\n",
    "# test_num  = 0\n",
    "# test_id   = []\n",
    "# #test_image = []\n",
    "# test_probability_label = [] # 8bit\n",
    "# test_probability_mask  = [] # 8bit\n",
    "# test_truth_label = []\n",
    "# test_truth_mask  = []\n",
    "\n",
    "\n",
    "\n",
    "# for image_id,images in tqdm.tqdm_notebook(testset):\n",
    "#     images = images.cuda()\n",
    "# #             truth_label = truth_label.cuda()\n",
    "# #             truth_mask = truth_mask.cuda()\n",
    "#     batch_size,C,H,W = images.shape\n",
    "# #            print(images.shape)\n",
    "# #            print(masks.shape)\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "#         num_augment = 0\n",
    "#         if 1: #  null\n",
    "#             logit =  data_parallel(model,images)  #net(input)\n",
    "#             probability = torch.softmax(logit,1)\n",
    "\n",
    "#             probability_mask = sharpen(probability,0)\n",
    "#             num_augment+=1\n",
    "\n",
    "# #                if 'flip_lr' in augment:\n",
    "# #                    logit = data_parallel(net,torch.flip(input,dims=[3]))\n",
    "# #                    probability  = torch.softmax(torch.flip(logit,dims=[3]),1)\n",
    "# #    \n",
    "# #                    probability_mask += sharpen(psrobability)\n",
    "# #                    num_augment+=1\n",
    "# #    \n",
    "# #                if 'flip_ud' in augment:\n",
    "# #                    logit = data_parallel(net,torch.flip(input,dims=[2]))\n",
    "# #                    probability = torch.softmax(torch.flip(logit,dims=[2]),1)\n",
    "# #    \n",
    "# #                    probability_mask += sharpen(probability)\n",
    "# #                    num_augment+=1\n",
    "# #    \n",
    "#         #---\n",
    "#         probability_mask = probability_mask/num_augment\n",
    "# #         probability_label = probability_mask_to_probability_label(probability_mask)\n",
    "\n",
    "#     #---\n",
    "# #             truth_label = truth_label.data.cpu().numpy().astype(np.uint8)\n",
    "# #             truth_mask  = truth_mask.data.cpu().numpy().astype(np.uint8)\n",
    "#     probability_mask = (probability_mask.data.cpu().numpy()*255).astype(np.uint8)\n",
    "# #     probability_label = (probability_label.data.cpu().numpy()).astype(np.uint8)\n",
    "\n",
    "#     test_probability_mask.extend(probability_mask)\n",
    "# #     test_probability_label.extend(probability_label)\n",
    "#     test_id.extend(image_id)\n",
    "# #             test_truth_mask.append(truth_mask)\n",
    "# #             test_truth_label.append(truth_label)\n",
    "#     test_num += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# test_id = np.array(test_id)\n",
    "# test_probability_mask = np.array(test_probability_mask)\n",
    "# test_probability_mask = np.concatenate(test_probability_mask)\n",
    "# test_probability_label = np.concatenate(test_probability_label)\n",
    "# test_id = np.concatenate(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# test_probability_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#         print(test_id.shape)\n",
    "#         print(test_probability_label.shape)\n",
    "#         print(test_probability_mask.shape)\n",
    "#         test_truth_mask  = np.concatenate(test_truth_mask)\n",
    "#         test_truth_label = np.concatenate(test_truth_label)\n",
    "\n",
    "\n",
    "#         value = np.max(test_probability_mask,1,keepdims=True)\n",
    "#         value = test_probability_mask*(value==test_probability_mask)\n",
    "# test_probability_mask = test_probability_mask[:,1:] #remove background class\n",
    "\n",
    "#         index = np.ones((len(test_id),4,256,1600),np.uint8)*np.array([1,2,3,4],np.uint8).reshape(1,4,1,1)\n",
    "#         truth_mask = truth_mask==index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# res = []\n",
    "# for p, file in zip(test_probability_mask, test_id):\n",
    "#     for i in range(4):\n",
    "#         p_channel = p[i]\n",
    "#         imageid_classid = file+'_'+str(i+1)\n",
    "#         p_channel = (p_channel>(threshold_mask_pixel[i]*255)).astype(np.uint8)\n",
    "#         if p_channel.sum() < threshold_mask_size[i]:\n",
    "#             p_channel = np.zeros(p_channel.shape, dtype=p_channel.dtype)\n",
    "\n",
    "#         res.append({\n",
    "#             'ImageId_ClassId': imageid_classid,\n",
    "#             'EncodedPixels': run_length_encode(p_channel)\n",
    "#         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(res)\n",
    "# df = df.fillna('')\n",
    "# df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# sum(df.EncodedPixels != '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
